{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir already exists\n",
      "timestep 0\n",
      "timestep 1\n",
      "timestep 2\n",
      "timestep 3\n",
      "timestep 4\n",
      "timestep 0\n",
      "timestep 1\n",
      "timestep 2\n",
      "timestep 3\n",
      "timestep 4\n",
      "norming the grads\n",
      "grads are\n",
      "[<tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_0:0' shape=(103, 100) dtype=float64>, <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_1:0' shape=(100,) dtype=float64>, <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_2:0' shape=(100, 3) dtype=float64>, <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_3:0' shape=(3,) dtype=float64>]\n",
      "1001\n",
      "499\n",
      "INFO:tensorflow:Restoring parameters from ./summaries/100_state_1000_samples_100_batch_5_ops_np_add_no_clip_pred_work_seed/model/./-50\n",
      "Sotfmax test loss\t 7.56226656754e+16\n",
      "Hardmax test loss\t 1.37213534547e+12\n",
      "[array([[-0.08 ,  0.024,  0.085],\n",
      "       [-0.04 , -0.099, -0.146],\n",
      "       [-0.004, -0.183, -0.171],\n",
      "       [-0.274, -0.135, -0.066],\n",
      "       [-0.283, -0.191,  0.016],\n",
      "       [-0.12 , -0.132, -0.066],\n",
      "       [-0.218, -0.244, -0.074],\n",
      "       [-0.066, -0.135, -0.26 ],\n",
      "       [-0.141,  0.047, -0.037],\n",
      "       [-0.088, -0.037, -0.188],\n",
      "       [-0.142,  0.048, -0.01 ],\n",
      "       [-0.071, -0.122, -0.093],\n",
      "       [-0.196, -0.13 ,  0.01 ],\n",
      "       [-0.294,  0.031, -0.132],\n",
      "       [-0.265, -0.021, -0.113],\n",
      "       [-0.047, -0.012, -0.096],\n",
      "       [-0.242, -0.079, -0.097],\n",
      "       [-0.145,  0.022, -0.211],\n",
      "       [-0.192, -0.138, -0.226],\n",
      "       [-0.243, -0.147,  0.008],\n",
      "       [-0.188,  0.013, -0.261],\n",
      "       [-0.085, -0.132, -0.192],\n",
      "       [-0.182, -0.16 , -0.099],\n",
      "       [-0.115, -0.055, -0.204],\n",
      "       [-0.049, -0.287, -0.059],\n",
      "       [ 0.038, -0.237, -0.071],\n",
      "       [-0.218, -0.017,  0.053],\n",
      "       [-0.109, -0.034, -0.002],\n",
      "       [-0.004, -0.028, -0.099],\n",
      "       [-0.067,  0.015, -0.17 ],\n",
      "       [ 0.002, -0.236, -0.166],\n",
      "       [-0.223, -0.087,  0.036],\n",
      "       [-0.145, -0.11 , -0.009],\n",
      "       [-0.033, -0.024, -0.21 ],\n",
      "       [-0.024, -0.226, -0.01 ],\n",
      "       [-0.245, -0.034, -0.023],\n",
      "       [-0.015, -0.276, -0.058],\n",
      "       [-0.059, -0.182, -0.054],\n",
      "       [-0.156, -0.027, -0.13 ],\n",
      "       [-0.052, -0.221, -0.106],\n",
      "       [-0.332, -0.251, -0.032],\n",
      "       [-0.179, -0.004, -0.042],\n",
      "       [ 0.116,  0.033, -0.204],\n",
      "       [-0.291, -0.117, -0.121],\n",
      "       [-0.112,  0.014,  0.015],\n",
      "       [-0.05 , -0.069,  0.018],\n",
      "       [-0.29 ,  0.053, -0.072],\n",
      "       [-0.264, -0.036,  0.019],\n",
      "       [-0.173, -0.01 , -0.149],\n",
      "       [-0.156, -0.17 ,  0.007],\n",
      "       [ 0.035, -0.171, -0.197],\n",
      "       [-0.048, -0.139,  0.012],\n",
      "       [-0.164,  0.007, -0.097],\n",
      "       [-0.205, -0.157, -0.071],\n",
      "       [-0.053, -0.034,  0.049],\n",
      "       [-0.219,  0.094, -0.225],\n",
      "       [-0.253, -0.182, -0.087],\n",
      "       [-0.187, -0.064,  0.019],\n",
      "       [-0.192,  0.045, -0.086],\n",
      "       [-0.029, -0.172, -0.04 ],\n",
      "       [-0.091, -0.076, -0.112],\n",
      "       [-0.093, -0.026, -0.117],\n",
      "       [ 0.021,  0.007, -0.148],\n",
      "       [-0.042, -0.052, -0.137],\n",
      "       [-0.055,  0.087, -0.107],\n",
      "       [-0.11 , -0.261, -0.048],\n",
      "       [-0.172, -0.101, -0.132],\n",
      "       [-0.366,  0.004, -0.079],\n",
      "       [-0.234, -0.038, -0.118],\n",
      "       [-0.135,  0.032, -0.233],\n",
      "       [-0.062, -0.077, -0.038],\n",
      "       [-0.17 , -0.26 , -0.011],\n",
      "       [-0.094, -0.079, -0.154],\n",
      "       [-0.028, -0.142, -0.199],\n",
      "       [-0.076, -0.118, -0.149],\n",
      "       [ 0.069, -0.061, -0.12 ],\n",
      "       [-0.054,  0.014, -0.186],\n",
      "       [-0.216, -0.047, -0.095],\n",
      "       [-0.219, -0.189, -0.078],\n",
      "       [-0.053,  0.093, -0.124],\n",
      "       [-0.109,  0.021,  0.036],\n",
      "       [-0.034, -0.06 , -0.214],\n",
      "       [-0.198, -0.058,  0.021],\n",
      "       [-0.122, -0.118, -0.051],\n",
      "       [-0.07 , -0.145, -0.215],\n",
      "       [-0.075, -0.063, -0.017],\n",
      "       [ 0.017, -0.093, -0.143],\n",
      "       [-0.072,  0.036,  0.006],\n",
      "       [-0.317, -0.279, -0.094],\n",
      "       [-0.017, -0.061, -0.01 ],\n",
      "       [-0.005, -0.194, -0.06 ],\n",
      "       [-0.011, -0.133, -0.077],\n",
      "       [-0.008, -0.123,  0.03 ],\n",
      "       [-0.128, -0.101, -0.098],\n",
      "       [-0.133, -0.001, -0.234],\n",
      "       [-0.086,  0.028, -0.273],\n",
      "       [-0.218, -0.11 , -0.077],\n",
      "       [-0.059,  0.039, -0.12 ],\n",
      "       [-0.124, -0.067, -0.218],\n",
      "       [-0.107,  0.085,  0.008]])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python import debug as tf_debug\n",
    "from numpy.random import RandomState\n",
    "import random\n",
    "import time\n",
    "import threading \n",
    "from tensorflow.python.client import timeline\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "#model flags\n",
    "tf.flags.DEFINE_boolean(\"debug\", False, \"weather run in a dubg mode\")\n",
    "tf.flags.DEFINE_boolean(\"norm\", True, \"weather to norm grads\")\n",
    "tf.flags.DEFINE_integer(\"seed\", round(random.random()*100000), \"the global simulation seed for np and tf\")\n",
    "tf.flags.DEFINE_string(\"name\", \"predef_sim_name\" , \"name of the simulation\")\n",
    "\n",
    "datatype = tf.float64\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "\n",
    "#set random seed\n",
    "tf.set_random_seed(FLAGS.seed)\n",
    "\n",
    "def variable_summaries(var):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "  with tf.name_scope(var.name.replace(\":\",\"_\")):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    tf.summary.scalar('stddev', tf.sqrt(tf.reduce_mean(tf.square(var - mean))))\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)\n",
    "\n",
    "def write_no_tf_summary(writer, tag, val, step):\n",
    "   summary=tf.Summary()\n",
    "   summary.value.add(tag=tag, simple_value = val)\n",
    "   writer.add_summary(summary, step)\n",
    "    \n",
    "def split_train_test (x, y , test_ratio):\n",
    "    \n",
    "    if y.shape != x.shape:\n",
    "        raise Exception('Model expects x and y shapes to be the same')\n",
    "    \n",
    "    test_len  = int(x.shape[0]*test_ratio)\n",
    "    train_len = x.shape[0] - test_len\n",
    "\n",
    "    x_train = x[0:train_len][:]\n",
    "    x_test  = x[-test_len:][:]\n",
    "    y_train = y[0:train_len][:]\n",
    "    y_test  = y[-test_len:][:]\n",
    "    \n",
    "    print(train_len)\n",
    "    print(test_len)\n",
    "\n",
    "    train_shape = (train_len, x.shape[1])\n",
    "    test_shape = (test_len, x.shape[1])\n",
    "    \n",
    "    if y_train.shape != train_shape or x_train.shape != train_shape or x_test.shape != test_shape or y_test.shape != test_shape:\n",
    "        raise Exception('One of the conversion test/train shapes gone wrong')\n",
    "    \n",
    "    return  x_train, x_test, y_train, y_test\n",
    "\n",
    "#helpder func\n",
    "def get_time_hhmmss(dif):\n",
    "    m, s = divmod(dif, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    time_str = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "    return time_str\n",
    "\n",
    "\n",
    "#sample gen functions\n",
    "def np_add(vec):\n",
    "    return reduce((lambda x, y: x + y),vec)\n",
    "\n",
    "def np_mult(vec):\n",
    "    return reduce((lambda x, y: x * y),vec)\n",
    "\n",
    "def np_stall(vec):\n",
    "    return vec\n",
    "\n",
    "def samples_generator(fn, shape, rng, seed):\n",
    "    '''\n",
    "    Generate random samples for the model:\n",
    "    @fn - function to be applied on the input features to get the ouput\n",
    "    @shape - shape of the features matrix (num_samples, num_features)\n",
    "    @rng - range of the input features to be generated within (a,b)\n",
    "    Outputs a tuple of input and output features matrix\n",
    "    '''\n",
    "    prng = RandomState(seed)\n",
    "    x = (rng[1] - rng[0]) * prng.random_sample(shape) + rng[0]\n",
    "    y = np.apply_along_axis(fn, 1, x).reshape((shape[0],-1))\n",
    "    z = np.zeros((shape[0],shape[1] - y.shape[1]))\n",
    "    y = np.concatenate((y, z), axis=1)\n",
    "    \n",
    "    return x,y\n",
    "\n",
    "#configuraion constants\n",
    "total_num_epochs = 10000000\n",
    "iters_per_epoch = 1\n",
    "num_epochs = total_num_epochs // iters_per_epoch\n",
    "state_size = 100\n",
    "num_of_operations = 3\n",
    "max_output_ops = 5\n",
    "num_features = 3\n",
    "num_samples = 1500\n",
    "samples_value_rng = (-100, 100)\n",
    "test_ratio = 0.33333333333\n",
    "batch_size  = 100\n",
    "param_init = 0.1\n",
    "learning_rate = 0.005\n",
    "epsilon=1e-6\n",
    "grad_norm = 10e1\n",
    "seed = 70948\n",
    "train_fn = np_add\n",
    "name = FLAGS.name\n",
    "norm = FLAGS.norm\n",
    "\n",
    "#dumpl globals\n",
    "try:\n",
    "    os.mkdir('./summaries/' + FLAGS.name)\n",
    "except FileExistsError as err:\n",
    "    print(\"Dir already exists\")\n",
    "\n",
    "stdout_org = sys.stdout\n",
    "sys.stdout = open('./summaries/' + FLAGS.name  + '/globals.txt', 'w')\n",
    "print(globals())\n",
    "sys.stdout = stdout_org\n",
    "\n",
    "#model operations\n",
    "def tf_multiply(inpt):\n",
    "    return tf.reshape( tf.reduce_prod(inpt, axis = 1, name = \"tf_mult\"), [batch_size, -1], name = \"tf_mult_reshape\")\n",
    "\n",
    "def tf_add(inpt):\n",
    "    return  tf.reshape( tf.reduce_sum(inpt, axis = 1, name = \"tf_add\"), [batch_size, -1], name = \"tf_add_reshape\")\n",
    "\n",
    "def tf_stall(a):\n",
    "    return a\n",
    "\n",
    "\n",
    "#model constants\n",
    "dummy_matrix = tf.zeros([batch_size, num_features], dtype=datatype, name=\"dummy_constant\")\n",
    "\n",
    "#model placeholders\n",
    "batchX_placeholder = tf.placeholder(datatype, [batch_size, None], name=\"batchX\")\n",
    "batchY_placeholder = tf.placeholder(datatype, [batch_size, None], name=\"batchY\")\n",
    "\n",
    "init_state = tf.placeholder(datatype, [batch_size, state_size], name=\"init_state\")\n",
    "\n",
    "#model parameters\n",
    "W = tf.Variable(tf.truncated_normal([state_size+num_features, state_size], -1*param_init, param_init, dtype=datatype), dtype=datatype, name=\"W\")\n",
    "b = tf.Variable(np.zeros((state_size)), dtype=datatype, name=\"b\")\n",
    "variable_summaries(W)\n",
    "variable_summaries(b)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([state_size, num_of_operations], -1*param_init, param_init, dtype=datatype),dtype=datatype, name=\"W2\")\n",
    "b2 = tf.Variable(np.zeros((num_of_operations)), dtype=datatype, name=\"b2\")\n",
    "variable_summaries(W2)\n",
    "variable_summaries(b2)\n",
    "\n",
    "    #forward pass\n",
    "def run_forward_pass(mode=\"train\"):\n",
    "    current_state = init_state\n",
    "\n",
    "    output = batchX_placeholder\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    softmaxes = []\n",
    "    \n",
    "    #printtf = tf.Print(output, [output], message=\"Strated cycle\")\n",
    "    #output = tf.reshape( printtf, [batch_size, -1], name = \"dummu_rehap\")\n",
    "    \n",
    "    for timestep in range(max_output_ops):\n",
    "        print(\"timestep \" + str(timestep))\n",
    "        current_input = output\n",
    "\n",
    "\n",
    "\n",
    "        input_and_state_concatenated = tf.concat([current_input, current_state], 1, name=\"concat_input_state\")  # Increasing number of columns\n",
    "        next_state = tf.tanh(tf.add(tf.matmul(input_and_state_concatenated, W, name=\"input-state_mult_W\"), b, name=\"add_bias\"), name=\"tanh_next_state\")  # Broadcasted addition\n",
    "        #next_state = tf.nn.relu(tf.add(tf.matmul(input_and_state_concatenated, W, name=\"input-state_mult_W\"), b, name=\"add_bias\"), name=\"relu_next-state\")  # Broadcasted addition\n",
    "        current_state = next_state\n",
    "\n",
    "        #calculate softmax and produce the mask of operations\n",
    "        logits = tf.add(tf.matmul(next_state, W2, name=\"state_mul_W2\"), b2, name=\"add_bias2\") #Broadcasted addition\n",
    "        softmax = tf.nn.softmax(logits, name=\"get_softmax\")\n",
    "        #argmax = tf.argmax(softmax, 1)\n",
    "        '''\n",
    "        print(logits)\n",
    "        print(softmax)\n",
    "        print(argmax)\n",
    "        '''\n",
    "        #perform ops\n",
    "        add   = tf_add(current_input)\n",
    "        mult  = tf_multiply(current_input)\n",
    "        stall = tf_stall(current_input)\n",
    "        #add = tf.reshape( tf.reduce_prod(current_input, axis = 1), [batch_size, -1])\n",
    "        #mult = tf.reshape( tf.reduce_sum(current_input, axis = 1), [batch_size, -1])\n",
    "        #stall = current_input\n",
    "        #values = tf.concat([add, mult, stall], 1)\n",
    "        #values = tf.concat([add, mult, stall], 1, name=\"concact_op_values\")\n",
    "        #values = tf.cast(values,dtype=datatype)\n",
    "        #get softmaxes for operations\n",
    "        #add_softmax = tf.slice(softmax, [0,0], [batch_size,1])\n",
    "        #mult_softmax = tf.slice(softmax, [0,1], [batch_size,1])\n",
    "        #stall_softmax = tf.slice(softmax, [0,2], [batch_size,1])\n",
    "        #produce output matrix\n",
    "        #onehot  = tf.one_hot(argmax_dum, num_of_operations)\n",
    "        #stall_width = tf.shape(stall)[1]\n",
    "        #stall_select = tf.slice(onehot, [0,2], [batch_size,1])\n",
    "        #mask_arr = [onehot]\n",
    "        #for i in range(num_features-1):\n",
    "        #    mask_arr.append(stall_select)\n",
    "        #mask = tf.concat(mask_arr, 1)\n",
    "        #argmax = tf.reshape( softmax, [batch_size, -1])\n",
    "        #mask = onehot\n",
    "        #mask = tf.cast(mask, dtype=datatype)\n",
    "        #mask = tf.cast(mask, tf.bool)\n",
    "        #apply mask\n",
    "        #output = tf.boolean_mask(values,mask)\n",
    "        #in test change to hardmax\n",
    "        if mode is \"test\":\n",
    "            argmax  = tf.argmax(softmax, 1, )\n",
    "            softmax  = tf.one_hot(argmax, num_of_operations, dtype=datatype)\n",
    "        #in the train mask = saturated softmax for all ops. in test change it to onehot(hardmax)\n",
    "        add_softmax   = tf.slice(softmax, [0,0], [batch_size,1], name=\"slice_add_softmax_val\")\n",
    "        mult_softmax  = tf.slice(softmax, [0,1], [batch_size,1], name=\"slice_mult_softmax_val\")\n",
    "        stall_softmax = tf.slice(softmax, [0,2], [batch_size,1], name=\"stall_mult_softmax_val\")\n",
    "\n",
    "        add_width   = tf.shape(add, name=\"add_op_shape\")[1]\n",
    "        mult_width  = tf.shape(mult, name=\"mult_op_shape\")[1]\n",
    "        stall_width = tf.shape(stall, name=\"stall_op_shape\")[1]\n",
    "\n",
    "\n",
    "        add_final   = tf.multiply(add, add_softmax, name=\"mult_add_softmax\")\n",
    "        mult_final  = tf.multiply(mult,mult_softmax, name=\"mult_mult_softmax\")\n",
    "        stall_final = tf.multiply(stall, stall_softmax, name=\"mult_stall_softmax\")\n",
    "\n",
    "        ##conact add and mult results with zeros matrix\n",
    "        add_final = tf.concat([add_final, tf.slice(dummy_matrix, [0,0], [batch_size, num_features - add_width], name=\"slice_dum_add\")], 1, name=\"concat_add_op_dummy_zeros\") \n",
    "        mult_final = tf.concat([mult_final, tf.slice(dummy_matrix, [0,0], [batch_size, num_features - mult_width], name=\"slice_dum_mult\")], 1, name=\"concat_mult_op_dummy_zeros\") \n",
    "\n",
    "\n",
    "        output = tf.add(add_final, mult_final, name=\"add_final_op_mult_add\")\n",
    "        output =  tf.add(output, stall_final, name=\"add_final_op_stall\")\n",
    "        outputs.append(output)\n",
    "        softmaxes.append(softmax)\n",
    "    #printtf = tf.Print(output, [output], message=\"Finished cycle\")\n",
    "    #output = tf.reshape( printtf, [batch_size, -1], name = \"dummu_rehap\")\n",
    "    return output, current_state, softmax, outputs, softmaxes\n",
    "\n",
    "#cost function\n",
    "def calc_loss(output):\n",
    "    #reduced_output = tf.reshape( tf.reduce_sum(output, axis = 1, name=\"red_output\"), [batch_size, -1], name=\"resh_red_output\")\n",
    "    math_error = tf.multiply(tf.constant(0.5, dtype=datatype), tf.square(tf.subtract(output , batchY_placeholder, name=\"sub_otput_batchY\"), name=\"squar_error\"), name=\"mult_with_0.5\")\n",
    "    \n",
    "    total_loss = tf.reduce_sum(math_error, name=\"red_total_loss\")\n",
    "    return total_loss, math_error\n",
    "\n",
    "output_train, current_state_train, softmax_train, outputs_train, softmaxes_train = run_forward_pass(mode = \"train\")\n",
    "total_loss_train, math_error_train = calc_loss(output_train)\n",
    "\n",
    "output_test, current_state_test, softmax_test, outputs_test, softmaxes_test = run_forward_pass(mode = \"test\")\n",
    "total_loss_test, math_error_test = calc_loss(output_test)\n",
    "\n",
    "grads_raw = tf.gradients(output_train, [W,b,W2,b2], name=\"comp_gradients\")\n",
    "\n",
    "#clip gradients by value and add summaries\n",
    "if norm:\n",
    "    print(\"norming the grads\")\n",
    "    grads, norms = tf.clip_by_global_norm(grads_raw, grad_norm)\n",
    "    variable_summaries(norms)\n",
    "else:\n",
    "    grads = grads_raw\n",
    "\n",
    "for grad in grads: variable_summaries(grad)\n",
    "\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate, epsilon ,name=\"AdamOpt\").apply_gradients(zip(grads, [W,b,W2,b2]), name=\"min_loss\")\n",
    "print(\"grads are\")\n",
    "print(grads)\n",
    "\n",
    "#pre training setting\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "#train_fn = np_mult\n",
    "#train_fn = np_stall\n",
    "x,y = samples_generator(train_fn, (num_samples, num_features) , samples_value_rng, seed)\n",
    "x_train, x_test, y_train, y_test = split_train_test (x, y , test_ratio)\n",
    "num_batches = x_train.shape[0]//batch_size\n",
    "num_test_batches = x_test.shape[0]//batch_size\n",
    "#model training\n",
    "\n",
    "#create a saver to save the trained model\n",
    "saver=tf.train.Saver(var_list=tf.trainable_variables())\n",
    "\n",
    "#Enable jit\n",
    "config = tf.ConfigProto()\n",
    "config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    loss_list_train_soft = [0,0]\n",
    "    loss_list_train_hard = [0,0]\n",
    "    loss_list_test_soft = [0,0]\n",
    "    loss_list_test_hard = [0,0]\n",
    "    # Merge all the summaries and write them out \n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter('./summaries/' + FLAGS.name ,sess.graph)\n",
    "    ##enable debugger if necessary\n",
    "    if (FLAGS.debug):\n",
    "        print(\"Running in a debug mode\")\n",
    "        sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "        sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    path = './summaries/100_state_1000_samples_100_batch_5_ops_np_add_no_clip_pred_work_seed/model/.'\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(path))\n",
    "    \n",
    "    _current_state_train = np.zeros((batch_size, state_size))\n",
    "    _current_state_test = np.zeros((batch_size, state_size))\n",
    "    for batch_idx in range(num_test_batches):\n",
    "                start_idx = batch_size * batch_idx\n",
    "                end_idx   = batch_size * batch_idx + batch_size\n",
    "\n",
    "                batchX = x_test[start_idx:end_idx]\n",
    "                batchY = y_test[start_idx:end_idx]\n",
    "                \n",
    "                _total_loss_train, _current_state_train = sess.run([total_loss_train, current_state_train],\n",
    "                    feed_dict={\n",
    "                        init_state:_current_state_train,\n",
    "                        batchX_placeholder:batchX,\n",
    "                        batchY_placeholder:batchY\n",
    "                    })\n",
    "                loss_list_test_soft.append(_total_loss_train)\n",
    "                \n",
    "                _total_loss_test, _current_state_test = sess.run([total_loss_test, current_state_test],\n",
    "                    feed_dict={\n",
    "                        init_state:_current_state_test,\n",
    "                        batchX_placeholder:batchX,\n",
    "                        batchY_placeholder:batchY\n",
    "                    })\n",
    "                loss_list_test_hard.append(_total_loss_test)\n",
    "    print(\"Sotfmax test loss\\t\", reduce(lambda x, y: x+y, loss_list_test_soft))\n",
    "    print(\"Hardmax test loss\\t\", reduce(lambda x, y: x+y, loss_list_test_hard))\n",
    "    W2 =  sess.run([W2])\n",
    "    print(W2)\n",
    "    #saver.restore(sess, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
