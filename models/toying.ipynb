{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed 67121\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from data_gen import *\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "#gloabal_seed =  round(random.random()*100000)\n",
    "gloabal_seed =  67121\n",
    "print (\"seed\", gloabal_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 4\n",
    "batch_size = 1\n",
    "state_size = 2\n",
    "datatype = tf.float64\n",
    "num_of_operations = 3\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(gloabal_seed)\n",
    "tf.set_random_seed(gloabal_seed)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x,y = samples_generator(np_center, (batch_size, num_features) , (-100,100), gloabal_seed)\n",
    "\n",
    "batchX = tf.Variable(x, dtype=datatype)\n",
    "state  = tf.Variable(np.random.rand(batch_size, state_size), dtype=datatype)\n",
    "W      = tf.Variable(np.random.rand(state_size+num_features, state_size), dtype=datatype, name=\"W\")\n",
    "W2     = tf.Variable(np.random.rand(state_size, num_of_operations),dtype=datatype, name=\"W2\")\n",
    "W3     = tf.Variable(np.random.rand(num_of_operations, num_features),dtype=datatype, name=\"W3\")\n",
    "\n",
    "batchX.initializer.run()\n",
    "state.initializer.run()\n",
    "W.initializer.run()\n",
    "W2.initializer.run()\n",
    "W3.initializer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([-63.75753726,  99.2127466 ,   7.23473607, -90.12625023]),\n",
       "  array([ -51.89846105,  111.0718228 ,   19.09381227,  -78.26717402]))]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def custom_softmax(x, base = 1):\n",
    "    maxx = tf.reduce_max(x, axis=1, keep_dims=True)\n",
    "    print(\"maxx\")\n",
    "    print(maxx.eval())\n",
    "    maxg = (x - maxx)\n",
    "    print(\"maxg\")\n",
    "    print(maxg.eval())\n",
    "    #powx = tf.pow(maxg, tf.cast(-base, datatype))\n",
    "    maxg =tf.nn.relu(maxg)\n",
    "    powx = maxg\n",
    "    for i in range(base):\n",
    "        powx = tf.sqrt(powx)\n",
    "    #reduced  = tf.reduce_sum(powx, axis=1, keep_dims=True)\n",
    "\n",
    "    return powx\n",
    "#tf.maximum\n",
    "\n",
    "\n",
    "def tf_softmax(x):\n",
    "    sum_x = tf.reduce_sum(tf.exp(x))\n",
    "    print(\"sum_x\")\n",
    "    print(sum_x.eval())\n",
    "    soft_x = tf.exp(x)/sum_x\n",
    "    print(\"soft_x\")\n",
    "    print(soft_x.eval())\n",
    "    return soft_x\n",
    "           \n",
    "'''\n",
    "def custom_softmax( x,base = 380):\n",
    "        maxx = tf.reduce_max(x, axis=1, keep_dims=True)\n",
    "        powx = tf.pow(tf.cast(base, datatype), x-maxx)\n",
    "        reduced  = tf.reduce_sum(powx, axis=1, keep_dims=True)\n",
    "        maxg = powx/ reduced\n",
    "        return maxg\n",
    "'''\n",
    "\n",
    "def calc_gradW3(W3, softmax):\n",
    "    shape = W3.get_shape().as_list()\n",
    "    der = tf.Variable(np.zeros(shape), dtype=datatype)\n",
    "    der.initializer.run()\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            d_temp = tf.Variable(np.zeros(shape), dtype=datatype)\n",
    "            d_temp.initializer.run()\n",
    "            tf.assign( d_temp[i,j], 1).eval()\n",
    "            der_temp = tf.reduce_sum(- tf.matmul(softmax, d_temp))\n",
    "            tf.assign( der[i,j], der_temp).eval()\n",
    "    return der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_pass(batchX, state):\n",
    "    X_and_state_con = tf.concat([batchX, state], 1)\n",
    "    print(\"X_and_state_con\")\n",
    "    print(X_and_state_con.eval())\n",
    "    layer1          = tf.matmul(X_and_state_con, W)\n",
    "    print(\"layer1\")\n",
    "    print(layer1.eval())\n",
    "    #next_state      = tf.nn.sigmoid(layer1)\n",
    "    next_state      = tf.nn.relu(layer1)\n",
    "    print(\"next_state\")\n",
    "    print(next_state.eval())\n",
    "    state_dropped = tf.layers.dropout(next_state, 0, training = True)\n",
    "    #print(\"state_dropped\")\n",
    "    #print(state_dropped.eval())\n",
    "    logits = tf.matmul(state_dropped, W2)\n",
    "    print(\"logits\")\n",
    "    print(logits.eval())\n",
    "    #softmax = custom_softmax(logits, 1)\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "    #print(\"cust_softmax\")\n",
    "    #print(cust_softmax.eval())\n",
    "    print(\"softmax\")\n",
    "    print(softmax.eval())\n",
    "    #softmax_loss = skewed_sigmoid_der_tf(softmax, num_of_operations)\n",
    "    #print(\"softmax_l2_losts\")\n",
    "    #print(softmax_loss.eval())\n",
    "    next_x = tf.matmul(softmax, W3)\n",
    "    print(\"next_x\")\n",
    "    print(next_x.eval())\n",
    "    selection = tf.multinomial(softmax, 1)\n",
    "    #reshape = tf.reshape(selection , [batch_size, -1])\n",
    "    print(\"selection\")\n",
    "    sel  = selection.eval() \n",
    "    print(sel)\n",
    "    \n",
    "    label = tf.one_hot(sel, num_of_operations, dtype=datatype)\n",
    "    print(\"label\")\n",
    "    print(label.eval())    \n",
    "    loss = tf.nn.l2_loss(label - softmax)\n",
    "    print(\"loss\")\n",
    "    print(loss.eval())\n",
    "    #grads  = tf.gradients(loss, [W,W2,W3])\n",
    "    print(\"tf_discounted_epr\")\n",
    "    tf_discounted_epr = tf.cast( sel * 0.0, datatype)\n",
    "    print(tf_discounted_epr.eval())\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.01)\n",
    "    tf_grads = optimizer.compute_gradients(loss, var_list= [W,W2])\n",
    "    tf_grads_dics = optimizer.compute_gradients(loss, var_list= [W,W2], grad_loss=tf_discounted_epr)\n",
    "    train_op = optimizer.apply_gradients(tf_grads)\n",
    "    print(tf_grads)\n",
    "    print(\"weights\")\n",
    "    [print(w.eval()) for w in [W,W2]]\n",
    "    print(\"grads_tf\")\n",
    "    [print(grad[0].eval()) for grad in tf_grads]\n",
    "    print(\"grads_tf_disc\")\n",
    "    [print(grad[0].eval()) for grad in tf_grads_dics]\n",
    "    print(\"affter applying op\")\n",
    "    \n",
    "    [print(w.eval()) for w in [W,W2]]\n",
    "    #loss fn\n",
    "    #softmax = e^x/sum(e^x)\n",
    "    #loss = reduced (y - softmax(relu([x;state_prev] * W1) * W2) * W3)\n",
    "    #print(\"dev_loss_w3\")\n",
    "    #h = 0.001\n",
    "    #dev_loss_w3 = calc_gradW3(W3, softmax)\n",
    "    #print(dev_loss_w3.eval())\n",
    "    return next_x, next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_and_state_con\n",
      "[[-0.62286753  0.18856623  0.30670629]]\n",
      "layer1\n",
      "[[-0.00982747 -0.09161487]]\n",
      "next_state\n",
      "[[ 0.  0.]]\n",
      "logits\n",
      "[[ 0.  0.]]\n",
      "softmax\n",
      "[[ 0.5  0.5]]\n",
      "next_x\n",
      "[[ 0.70501444]]\n",
      "selection\n",
      "[[1]]\n",
      "label\n",
      "[[[ 0.  1.]]]\n",
      "loss\n",
      "0.25\n",
      "tf_discounted_epr\n",
      "[[ 0.]]\n",
      "[(<tf.Tensor 'gradients/MatMul_grad/tuple/control_dependency_1:0' shape=(3, 2) dtype=float64>, <tf.Variable 'W:0' shape=(3, 2) dtype=float64_ref>), (<tf.Tensor 'gradients/MatMul_1_grad/tuple/control_dependency_1:0' shape=(2, 2) dtype=float64>, <tf.Variable 'W2:0' shape=(2, 2) dtype=float64_ref>)]\n",
      "weights\n",
      "[[ 0.3751096   0.30023626]\n",
      " [ 0.27499064  0.26933108]\n",
      " [ 0.56067377  0.1454349 ]]\n",
      "[[ 0.74150469  0.09561272]\n",
      " [ 0.52968019  0.34472194]]\n",
      "grads_tf\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "grads_tf_disc\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "affter applying op\n",
      "[[ 0.3751096   0.30023626]\n",
      " [ 0.27499064  0.26933108]\n",
      " [ 0.56067377  0.1454349 ]]\n",
      "[[ 0.74150469  0.09561272]\n",
      " [ 0.52968019  0.34472194]]\n"
     ]
    }
   ],
   "source": [
    "state1 = run_pass(batchX, state)[1].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state2 = run_pass(batchX, state1)[1].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state3 = run_pass(batchX, state2)[1].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use derivative of sigmoid to penelise for values other than 1 or 0\n",
    "def sigmoid_der(x, scale = 10):\n",
    "    shifted_x = x - 0.5\n",
    "    scaled_x = scale*shifted_x\n",
    "    res = tf.sigmoid(scaled_x)*(1 - tf.sigmoid(scaled_x))\n",
    "    return (res - 0.01)*scale**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skewed_dist(x, scale=10):\n",
    "    mean = 0.5\n",
    "    str_d = mean/3.5\n",
    "    const = 1/tf.sqrt(2*math.pi*(str_d**2))\n",
    "    power = ((x-mean)**2) / (2*str_d**2)\n",
    "    return const*math.exp(-power)*scale**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skewed_sigmoid_der(x, scale = 10, num_ops = 3):\n",
    "    shifted_x = x - 0.5\n",
    "    scaled_x = scale*shifted_x\n",
    "    nom = tf.exp(-scaled_x)+num_ops*tf.exp(-num_ops*scaled_x)+(num_ops+1)*tf.exp(-(num_ops+1)*scaled_x)\n",
    "    denom = tf.square(tf.exp(-scaled_x) + 1)*tf.square(tf.exp(-num_ops*scaled_x) + 1)\n",
    "    res = nom/denom\n",
    "    return (res - 0.01)*scale**3\n",
    "\n",
    "def sigmoid(x, scale = 10, num_ops = 3):\n",
    "    shifted_x = x - 0.5\n",
    "    scaled_x = scale*shifted_x\n",
    "    return tf.sigmoid(scaled_x)*tf.sigmoid(10*scaled_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(fn, ops, subl):\n",
    "    t = np.arange(0.0, 1.01, 0.01)\n",
    "    s = [fn(val, ops) for val in t]\n",
    "    max_val = max(s)\n",
    "    probs = [1 - pen/max_val for pen in s]\n",
    "    fig, ax1 = plt.subplot(*subl)\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    ax1.plot(t, s, \"blue\")\n",
    "    ax1.xlabel('softmax')\n",
    "    ax1.ylabel('penalty')\n",
    "    \n",
    "    ax2.plot(t, probs, \"red\")\n",
    "    ax2.ylabel('Probability')\n",
    "\n",
    "    fig.title(\"Skewed sigmoid der per \"+str(ops)+ \"ops\")\n",
    "    fig.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skewed_sigmoid_der_py(x, num_ops = 3, scale = 10):\n",
    "    worst_case = 1 / num_ops\n",
    "    shifted_x = x - worst_case + 0.02\n",
    "    scaled_x = scale*shifted_x\n",
    "    nom = scale*math.exp(-scaled_x)+scale*num_ops*math.exp(-num_ops*scaled_x)\n",
    "    #denom = math.square(math.exp(-scaled_x) + 1)*math.square(math.exp(-(num_ops-1)*scaled_x) + 1)\n",
    "    denom = ((math.exp(-scaled_x) + 1)**2) * ((math.exp(-(num_ops-1)*scaled_x) + 1)**2)\n",
    "    res = nom/denom\n",
    "    return worst_case*((res)*scale**3)\n",
    "\n",
    "def skewed_sigmoid_der_tf(x, num_ops = 3, scale = 10):\n",
    "    worst_case = 1 / num_ops\n",
    "    shifted_x = x - worst_case + 0.02\n",
    "    scaled_x = scale*shifted_x\n",
    "    nom = scale*tf.exp(-scaled_x)+scale*num_ops*tf.exp(-num_ops*scaled_x)\n",
    "    denom = tf.square(tf.exp(-scaled_x) + 1)*tf.square(tf.exp(-(num_ops-1)*scaled_x) + 1)\n",
    "    res = nom/denom\n",
    "    return worst_case*((res - 0.01)*scale**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'AxesSubplot' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-aa6768ecdd10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mops\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskewed_sigmoid_der_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.30,\n\u001b[1;32m      5\u001b[0m                     wspace=0.20)\n",
      "\u001b[0;32m<ipython-input-53-b7ffe6cd709e>\u001b[0m in \u001b[0;36mplot\u001b[0;34m(fn, ops, subl)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpen\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmax_val\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpen\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msubl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0max2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwinx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'AxesSubplot' object is not iterable"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "for ops in range(6):\n",
    "    plot(skewed_sigmoid_der_py, ops+2, (2,3, ops+1))\n",
    "plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.30,\n",
    "                    wspace=0.20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(skewed_sigmoid_der_tf, 2)\n",
    "plot(skewed_sigmoid_der_tf, 3)\n",
    "plot(skewed_sigmoid_der_tf, 4)\n",
    "plot(skewed_sigmoid_der_tf, 5)\n",
    "plot(skewed_sigmoid_der_tf, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = np.arange(0.0, 1.01, 0.01)\n",
    "for val in t:\n",
    "    print (val, skewed_sigmoid_der_py(val, 5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sig_inv_loss(max_error_tot):\n",
    "    return (1 - sigmoid( (max_error_tot/50) - 10))\n",
    "\n",
    "def sig_inv_loss_tf(max_error_tot):\n",
    "    return (1 - tf.sigmoid( (max_error_tot/50) - 10)).eval()*0.5\n",
    "\n",
    "def log_loss_tf(max_error_tot):\n",
    "    return tf.log1p(max_error_tot*100).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = np.arange(0, 100000, 100.0)\n",
    "for val in t:\n",
    "    print (val, log_loss_tf(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradtest2(batchX, state):\n",
    "    X_and_state_con = tf.concat([batchX, state], 1)\n",
    "    print(\"X_and_state_con\")\n",
    "    print(X_and_state_con.eval())\n",
    "    layer1          = tf.matmul(X_and_state_con, W)\n",
    "    print(\"layer1\")\n",
    "    print(layer1.eval())\n",
    "    next_state      = tf.nn.sigmoid(layer1)\n",
    "    print(\"next_state\")\n",
    "    print(next_state.eval())\n",
    "    logits = tf.matmul(next_state, W2)\n",
    "    print(\"logits\")\n",
    "    print(logits.eval())\n",
    "    #softmax = tf_softmax(logits)\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "    #print(\"cust_softmax\")\n",
    "    #print(cust_softmax.eval())\n",
    "    print(\"softmax\")\n",
    "    print(softmax.eval())\n",
    "    #print(\"softmaxtf\")\n",
    "    #print(softmaxtf.eval())\n",
    "    \n",
    "    grads  = tf.gradients(softmax, logits)\n",
    "    alhpa_jacobean = calc_softmax_jacob(logits)\n",
    "    print(\"weights2\")\n",
    "    [print(w.eval()) for w in [W2]]\n",
    "    print(\"grads\")\n",
    "    [print(grad.eval()) for grad in grads]\n",
    "    print(\"alhpa_jacobean\")\n",
    "    print(alhpa_jacobean.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_softmax(val, x):\n",
    "    #print(\"single_softmax\")\n",
    "    sum_x = tf.reduce_sum(tf.exp(x))\n",
    "    #print(\"sum_x\")\n",
    "    #print(sum_x.eval())\n",
    "    soft_val = tf.exp(val)/sum_x\n",
    "    #print(\"soft_val\")\n",
    "    #print(soft_val.eval())\n",
    "    return soft_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_softmax_jacob(alpha):\n",
    "    leng = alpha.get_shape().as_list()[1]\n",
    "    shape=(leng,leng)\n",
    "    jacobian = tf.Variable(np.zeros(shape), dtype=datatype)\n",
    "    jacobian.initializer.run()\n",
    "    #print(\"calc_softmax_jacob\")\n",
    "    #print(\"aplha\")\n",
    "    #print(alpha.eval())\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            i_val = alpha[0][i].eval()\n",
    "            j_val = alpha[0][j].eval()\n",
    "            #print(i, j, i_val, j_val)\n",
    "            if i == j:\n",
    "                der = single_softmax(i_val,alpha) * (1 - single_softmax(j_val,alpha))\n",
    "            else:\n",
    "                der = -single_softmax(i_val,alpha) *  single_softmax(j_val,alpha)\n",
    "            tf.assign( jacobian[i,j], der.eval()).eval()\n",
    "    return jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_and_state_con\n",
      "[[ 0.65224106  0.82612053  0.32550767]]\n",
      "layer1\n",
      "[[ 0.82109083  0.47648064]]\n",
      "next_state\n",
      "[[ 0.69446784  0.61691649]]\n",
      "logits\n",
      "[[ 0.29210583  0.80191096  0.25311898]]\n",
      "softmax\n",
      "[[ 0.27573047  0.45908206  0.26518747]]\n",
      "weights2\n",
      "[[ 0.14580228  0.85268734  0.21834923]\n",
      " [ 0.30936251  0.33999257  0.16449952]]\n",
      "grads\n",
      "[[ 0.  0.  0.]]\n",
      "alhpa_jacobean\n",
      "[[ 0.19970318 -0.12658291 -0.07312027]\n",
      " [-0.12658291  0.24832572 -0.12174281]\n",
      " [-0.07312027 -0.12174281  0.19486307]]\n"
     ]
    }
   ],
   "source": [
    "gradtest2(batchX, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards( rewards):\n",
    "        \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    \n",
    "        #discount the rewards\n",
    "        discounted_r = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        \n",
    "        #if rewards did not swith, penelise the frst selection first\n",
    "        #ng = range(0, len(checked_rewards)) if rewards == checked_rewards else reversed(range(0, len(checked_rewards)))\n",
    "        for t in reversed(range(0, len(rewards))):          \n",
    "            running_add = running_add * 0.6 + rewards[t] #for all pos/negative rewards mean is always going to be bigger than the first reward, hence it will become positive when centered\n",
    "            print(running_add, rewards[t])\n",
    "            discounted_r[t] = running_add\n",
    "        #normalise rewards\n",
    "        \n",
    "        #dont scale but norm, as scaling might result into inversion of signs\n",
    "        normalised_r = (discounted_r - np.mean(discounted_r)) / (np.std(discounted_r) + 1e-10)\n",
    "        #normalised_r = discounted_r/ np.linalg.norm(discounted_r, 2)\n",
    "        return normalised_r, discounted_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "3000.0 3000.0\n",
      "4800.0 3000.0\n",
      "[ 4800.  3000.     0.     0.     0.]\n",
      "[ 1.62520902  0.72231512 -0.78250805 -0.78250805 -0.78250805]\n"
     ]
    }
   ],
   "source": [
    "normalised_r, discounted_r = discount_rewards( [3000.0, 3000.0,    0.0,     0.0,     0.0])\n",
    "print(discounted_r)\n",
    "print(normalised_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
