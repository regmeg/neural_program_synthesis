{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from data_gen import *\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_features = 1\n",
    "batch_size = 1\n",
    "state_size = 2\n",
    "datatype = tf.float64\n",
    "num_of_operations = 2\n",
    "\n",
    "\n",
    "gloabal_seed = round(random.random()*100000)\n",
    "np.random.seed(gloabal_seed)\n",
    "tf.set_random_seed(gloabal_seed)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x,y = samples_generator(np_add, (batch_size, num_features) , (-1,1), gloabal_seed)\n",
    "\n",
    "batchX = tf.Variable(x, dtype=datatype)\n",
    "state  = tf.Variable(np.random.rand(batch_size, state_size), dtype=datatype)\n",
    "W      = tf.Variable(np.random.rand(state_size+num_features, state_size), dtype=datatype, name=\"W\")\n",
    "W2     = tf.Variable(np.random.rand(state_size, num_of_operations),dtype=datatype, name=\"W2\")\n",
    "W3     = tf.Variable(np.random.rand(num_of_operations, num_features),dtype=datatype, name=\"W3\")\n",
    "\n",
    "batchX.initializer.run()\n",
    "state.initializer.run()\n",
    "W.initializer.run()\n",
    "W2.initializer.run()\n",
    "W3.initializer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 0.90576701]), array([ 0.90576701]))]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def custom_softmax(x, base = 1):\n",
    "    maxx = tf.reduce_max(x, axis=1, keep_dims=True)\n",
    "    print(\"maxx\")\n",
    "    print(maxx.eval())\n",
    "    maxg = (x - maxx)\n",
    "    print(\"maxg\")\n",
    "    print(maxg.eval())\n",
    "    #powx = tf.pow(maxg, tf.cast(-base, datatype))\n",
    "    maxg =tf.nn.relu(maxg)\n",
    "    powx = maxg\n",
    "    for i in range(base):\n",
    "        powx = tf.sqrt(powx)\n",
    "    #reduced  = tf.reduce_sum(powx, axis=1, keep_dims=True)\n",
    "\n",
    "    return powx\n",
    "#tf.maximum\n",
    "\n",
    "'''\n",
    "def custom_softmax( x,base = 380):\n",
    "        maxx = tf.reduce_max(x, axis=1, keep_dims=True)\n",
    "        powx = tf.pow(tf.cast(base, datatype), x-maxx)\n",
    "        reduced  = tf.reduce_sum(powx, axis=1, keep_dims=True)\n",
    "        maxg = powx/ reduced\n",
    "        return maxg\n",
    "'''\n",
    "\n",
    "def calc_gradW3(W3, softmax):\n",
    "    shape = W3.get_shape().as_list()\n",
    "    der = tf.Variable(np.zeros(shape), dtype=datatype)\n",
    "    der.initializer.run()\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            d_temp = tf.Variable(np.zeros(shape), dtype=datatype)\n",
    "            d_temp.initializer.run()\n",
    "            tf.assign( d_temp[i,j], 1).eval()\n",
    "            der_temp = tf.reduce_sum(- tf.matmul(softmax, d_temp))\n",
    "            tf.assign( der[i,j], der_temp).eval()\n",
    "    return der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_pass(batchX, state):\n",
    "    X_and_state_con = tf.concat([batchX, state], 1)\n",
    "    print(\"X_and_state_con\")\n",
    "    print(X_and_state_con.eval())\n",
    "    layer1          = tf.matmul(X_and_state_con, W)\n",
    "    print(\"layer1\")\n",
    "    print(layer1.eval())\n",
    "    #next_state      = tf.nn.sigmoid(layer1)\n",
    "    next_state      = tf.nn.relu(layer1)\n",
    "    print(\"next_state\")\n",
    "    print(next_state.eval())\n",
    "    state_dropped = tf.layers.dropout(next_state, 0, training = True)\n",
    "    #print(\"state_dropped\")\n",
    "    #print(state_dropped.eval())\n",
    "    logits = tf.matmul(state_dropped, W2)\n",
    "    print(\"logits\")\n",
    "    print(logits.eval())\n",
    "    #softmax = custom_softmax(logits, 1)\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "    #print(\"cust_softmax\")\n",
    "    #print(cust_softmax.eval())\n",
    "    print(\"softmax\")\n",
    "    print(softmax.eval())\n",
    "    #softmax_loss = skewed_sigmoid_der_tf(softmax, num_of_operations)\n",
    "    #print(\"softmax_l2_losts\")\n",
    "    #print(softmax_loss.eval())\n",
    "    next_x = tf.matmul(softmax, W3)\n",
    "    print(\"next_x\")\n",
    "    print(next_x.eval())\n",
    "    selection = tf.multinomial(softmax, 1)\n",
    "    #reshape = tf.reshape(selection , [batch_size, -1])\n",
    "    print(\"selection\")\n",
    "    sel  = selection.eval() \n",
    "    print(sel)\n",
    "    \n",
    "    label = tf.one_hot(sel, num_of_operations, dtype=datatype)\n",
    "    print(\"label\")\n",
    "    print(label.eval())    \n",
    "    loss = tf.nn.l2_loss(label - softmax)\n",
    "    print(\"loss\")\n",
    "    print(loss.eval())\n",
    "    #grads  = tf.gradients(loss, [W,W2,W3])\n",
    "    print(\"tf_discounted_epr\")\n",
    "    tf_discounted_epr = tf.cast( sel * 0.0, datatype)\n",
    "    print(tf_discounted_epr.eval())\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.01)\n",
    "    tf_grads = optimizer.compute_gradients(loss, var_list= [W,W2])\n",
    "    tf_grads_dics = optimizer.compute_gradients(loss, var_list= [W,W2], grad_loss=tf_discounted_epr)\n",
    "    train_op = optimizer.apply_gradients(tf_grads)\n",
    "    print(tf_grads)\n",
    "    print(\"weights\")\n",
    "    [print(w.eval()) for w in [W,W2]]\n",
    "    print(\"grads_tf\")\n",
    "    [print(grad[0].eval()) for grad in tf_grads]\n",
    "    print(\"grads_tf_disc\")\n",
    "    [print(grad[0].eval()) for grad in tf_grads_dics]\n",
    "    print(\"affter applying op\")\n",
    "    \n",
    "    [print(w.eval()) for w in [W,W2]]\n",
    "    #loss fn\n",
    "    #softmax = e^x/sum(e^x)\n",
    "    #loss = reduced (y - softmax(relu([x;state_prev] * W1) * W2) * W3)\n",
    "    #print(\"dev_loss_w3\")\n",
    "    #h = 0.001\n",
    "    #dev_loss_w3 = calc_gradW3(W3, softmax)\n",
    "    #print(dev_loss_w3.eval())\n",
    "    return next_x, next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_and_state_con\n",
      "[[ 0.90576701  0.95288351  0.29508586]]\n",
      "layer1\n",
      "[[ 0.92405152  1.8074639 ]]\n",
      "next_state\n",
      "[[ 0.92405152  1.8074639 ]]\n",
      "logits\n",
      "[[ 1.95624412  2.49685545]]\n",
      "softmax\n",
      "[[ 0.36804538  0.63195462]]\n",
      "next_x\n",
      "[[ 0.65242329]]\n",
      "selection\n",
      "[[0]]\n",
      "label\n",
      "[[[ 1.  0.]]]\n",
      "loss\n",
      "0.399366638283\n",
      "tf_discounted_epr\n",
      "[[ 0.]]\n",
      "[(<tf.Tensor 'gradients_51/MatMul_163_grad/tuple/control_dependency_1:0' shape=(3, 2) dtype=float64>, <tf.Variable 'W_1:0' shape=(3, 2) dtype=float64_ref>), (<tf.Tensor 'gradients_51/MatMul_164_grad/tuple/control_dependency_1:0' shape=(2, 2) dtype=float64>, <tf.Variable 'W2_1:0' shape=(2, 2) dtype=float64_ref>)]\n",
      "weights\n",
      "[[ 0.14246844  0.83353808]\n",
      " [ 0.59795942  0.96280752]\n",
      " [ 0.76324443  0.45759291]]\n",
      "[[ 0.62517075  0.86004648]\n",
      " [ 0.76270078  0.94172182]]\n",
      "grads_tf\n",
      "[[ 0.06253999  0.04766765]\n",
      " [ 0.06579321  0.05014724]\n",
      " [ 0.02037463  0.01552943]]\n",
      "[[-0.27164351  0.27164351]\n",
      " [-0.53134033  0.53134033]]\n",
      "grads_tf_disc\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "affter applying op\n",
      "[[ 0.14246844  0.83353808]\n",
      " [ 0.59795942  0.96280752]\n",
      " [ 0.76324443  0.45759291]]\n",
      "[[ 0.62517075  0.86004648]\n",
      " [ 0.76270078  0.94172182]]\n"
     ]
    }
   ],
   "source": [
    "state1 = run_pass(batchX, state)[1].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "state2 = run_pass(batchX, state1)[1].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "state3 = run_pass(batchX, state2)[1].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#use derivative of sigmoid to penelise for values other than 1 or 0\n",
    "def sigmoid_der(x, scale = 10):\n",
    "    shifted_x = x - 0.5\n",
    "    scaled_x = scale*shifted_x\n",
    "    res = tf.sigmoid(scaled_x)*(1 - tf.sigmoid(scaled_x))\n",
    "    return (res - 0.01)*scale**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def skewed_dist(x, scale=10):\n",
    "    mean = 0.5\n",
    "    str_d = mean/3.5\n",
    "    const = 1/tf.sqrt(2*math.pi*(str_d**2))\n",
    "    power = ((x-mean)**2) / (2*str_d**2)\n",
    "    return const*math.exp(-power)*scale**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def skewed_sigmoid_der(x, scale = 10, num_ops = 3):\n",
    "    shifted_x = x - 0.5\n",
    "    scaled_x = scale*shifted_x\n",
    "    nom = tf.exp(-scaled_x)+num_ops*tf.exp(-num_ops*scaled_x)+(num_ops+1)*tf.exp(-(num_ops+1)*scaled_x)\n",
    "    denom = tf.square(tf.exp(-scaled_x) + 1)*tf.square(tf.exp(-num_ops*scaled_x) + 1)\n",
    "    res = nom/denom\n",
    "    return (res - 0.01)*scale**3\n",
    "\n",
    "def sigmoid(x, scale = 10, num_ops = 3):\n",
    "    shifted_x = x - 0.5\n",
    "    scaled_x = scale*shifted_x\n",
    "    return tf.sigmoid(scaled_x)*tf.sigmoid(10*scaled_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot(fn, ops):\n",
    "    t = np.arange(0.0, 1.01, 0.01)\n",
    "    s = [fn(val, ops).eval() for val in t]\n",
    "    plt.plot(t, s)\n",
    "\n",
    "    plt.xlabel('softmax')\n",
    "    plt.ylabel('penalty')\n",
    "    plt.title(fn.__name__ + \" \"+str(ops))\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def skewed_sigmoid_der_py(x, num_ops = 3, scale = 10):\n",
    "    worst_case = 1 / num_ops\n",
    "    shifted_x = x - worst_case + 0.02\n",
    "    scaled_x = scale*shifted_x\n",
    "    nom = scale*math.exp(-scaled_x)+scale*num_ops*math.exp(-num_ops*scaled_x)\n",
    "    #denom = math.square(math.exp(-scaled_x) + 1)*math.square(math.exp(-(num_ops-1)*scaled_x) + 1)\n",
    "    denom = ((math.exp(-scaled_x) + 1)**2) * ((math.exp(-(num_ops-1)*scaled_x) + 1)**2)\n",
    "    res = nom/denom\n",
    "    return worst_case*((res)*scale**3)\n",
    "\n",
    "def skewed_sigmoid_der_tf(x, num_ops = 3, scale = 10):\n",
    "    worst_case = 1 / num_ops\n",
    "    shifted_x = x - worst_case + 0.02\n",
    "    scaled_x = scale*shifted_x\n",
    "    nom = scale*tf.exp(-scaled_x)+scale*num_ops*tf.exp(-num_ops*scaled_x)\n",
    "    denom = tf.square(tf.exp(-scaled_x) + 1)*tf.square(tf.exp(-(num_ops-1)*scaled_x) + 1)\n",
    "    res = nom/denom\n",
    "    return worst_case*((res - 0.01)*scale**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot(skewed_sigmoid_der_py, 2)\n",
    "plot(skewed_sigmoid_der_py, 3)\n",
    "plot(skewed_sigmoid_der_py, 4)\n",
    "plot(skewed_sigmoid_der_py, 5)\n",
    "plot(skewed_sigmoid_der_py, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(skewed_sigmoid_der_tf, 2)\n",
    "plot(skewed_sigmoid_der_tf, 3)\n",
    "plot(skewed_sigmoid_der_tf, 4)\n",
    "plot(skewed_sigmoid_der_tf, 5)\n",
    "plot(skewed_sigmoid_der_tf, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "t = np.arange(0.0, 1.01, 0.01)\n",
    "for val in t:\n",
    "    print (val, skewed_sigmoid_der_py(val, 5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sig_inv_loss(max_error_tot):\n",
    "    return (1 - sigmoid( (max_error_tot/50) - 10))\n",
    "\n",
    "def sig_inv_loss_tf(max_error_tot):\n",
    "    return (1 - tf.sigmoid( (max_error_tot/50) - 10)).eval()*0.5\n",
    "\n",
    "def log_loss_tf(max_error_tot):\n",
    "    return tf.log1p(max_error_tot*100).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "t = np.arange(0, 100000, 100.0)\n",
    "for val in t:\n",
    "    print (val, log_loss_tf(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
