{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import  tensorflow as tf\n",
    "import  numpy as np\n",
    "\n",
    "#define computational graph nodes (constants)\n",
    "node1 = tf.constant(5.0)\n",
    "node2 = tf.constant(8.0)\n",
    "node3 = tf.add(node1, node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(node3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.,   6.],\n",
       "       [ 13.,   5.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this define paretereised inputs (placeholders)\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b\n",
    "sess.run(adder_node, {a: [[3, 2], [10, 1]], b:[3, 4]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable([.3], tf.float32)\n",
    "b = tf.Variable([-.3], tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W*x + b\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.30000001,  0.60000002,  0.90000004], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(linear_model, {x:[1,2,3,4]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "sess.run(loss,{x:[1,2,3,4], y:[0,-1,-2,-3]})\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "x_train = [1,2,3,4]\n",
    "y_train = [0,-1,-2,-3]\n",
    "# training loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) # reset values to wrong\n",
    "for i in range(1000):\n",
    "  sess.run(train, {x:x_train, y:y_train})\n",
    "\n",
    "# evaluate training accuracy\n",
    "curr_W, curr_b, curr_loss  = sess.run([W, b, loss], {x:x_train, y:y_train})\n",
    "print(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9072\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for _ in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(500)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#more mature way of approaching MNIST\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y = tf.matmul(x,W) + b\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "  batch = mnist.train.next_batch(100)\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9199\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#convolutional net\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.14\n",
      "step 100, training accuracy 0.76\n",
      "step 200, training accuracy 0.94\n",
      "step 300, training accuracy 0.96\n",
      "step 400, training accuracy 0.94\n",
      "test accuracy 0.9444\n"
     ]
    }
   ],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(500):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%100 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import urllib\n",
    "import urllib.request as ur\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "IRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "if not os.path.exists(IRIS_TRAINING):\n",
    "  raw = ur.urlopen(IRIS_TRAINING_URL).read()\n",
    "  with open(IRIS_TRAINING,'wb') as f:\n",
    "    f.write(raw)\n",
    "\n",
    "if not os.path.exists(IRIS_TEST):\n",
    "  raw = ur.urlopen(IRIS_TEST_URL).read()\n",
    "  with open(IRIS_TEST,'wb') as f:\n",
    "    f.write(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load datasets.\n",
    "training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "    filename=IRIS_TRAINING,\n",
    "    target_dtype=np.int,\n",
    "    features_dtype=np.float32)\n",
    "test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "    filename=IRIS_TEST,\n",
    "    target_dtype=np.int,\n",
    "    features_dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f632fba52e8>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_master': '', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_model_dir': None, '_environment': 'local', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_keep_checkpoint_every_n_hours': 10000, '_is_chief': True, '_task_id': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_evaluation_master': ''}\n"
     ]
    }
   ],
   "source": [
    "# Specify that all features have real-value data\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n",
    "\n",
    "# Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n",
    "                                            hidden_units=[10, 20, 10],\n",
    "                                            n_classes=3,\n",
    "                                            model_dir=\"/tmp/iris_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define the training inputs\n",
    "def get_train_inputs():\n",
    "  x = tf.constant(training_set.data)\n",
    "  y = tf.constant(training_set.target)\n",
    "\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.4/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/iris_model/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 1.10782\n",
      "INFO:tensorflow:global_step/sec: 1512.85\n",
      "INFO:tensorflow:step = 101, loss = 0.164296 (0.065 sec)\n",
      "INFO:tensorflow:global_step/sec: 1700.89\n",
      "INFO:tensorflow:step = 201, loss = 0.0831136 (0.059 sec)\n",
      "INFO:tensorflow:global_step/sec: 1699.71\n",
      "INFO:tensorflow:step = 301, loss = 0.0670297 (0.059 sec)\n",
      "INFO:tensorflow:global_step/sec: 1749.65\n",
      "INFO:tensorflow:step = 401, loss = 0.0601996 (0.057 sec)\n",
      "INFO:tensorflow:global_step/sec: 360.406\n",
      "INFO:tensorflow:step = 501, loss = 0.0565224 (0.278 sec)\n",
      "INFO:tensorflow:global_step/sec: 1823.37\n",
      "INFO:tensorflow:step = 601, loss = 0.0539405 (0.055 sec)\n",
      "INFO:tensorflow:global_step/sec: 1784.79\n",
      "INFO:tensorflow:step = 701, loss = 0.0519839 (0.056 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.597\n",
      "INFO:tensorflow:step = 801, loss = 0.0504451 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 1283.23\n",
      "INFO:tensorflow:step = 901, loss = 0.0491599 (0.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 1818\n",
      "INFO:tensorflow:step = 1001, loss = 0.0480535 (0.055 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.3\n",
      "INFO:tensorflow:step = 1101, loss = 0.0470992 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 1712.42\n",
      "INFO:tensorflow:step = 1201, loss = 0.046231 (0.058 sec)\n",
      "INFO:tensorflow:global_step/sec: 1703.09\n",
      "INFO:tensorflow:step = 1301, loss = 0.045424 (0.059 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.012\n",
      "INFO:tensorflow:step = 1401, loss = 0.0447018 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 1617.48\n",
      "INFO:tensorflow:step = 1501, loss = 0.0440118 (0.061 sec)\n",
      "INFO:tensorflow:global_step/sec: 1745.93\n",
      "INFO:tensorflow:step = 1601, loss = 0.0433612 (0.057 sec)\n",
      "INFO:tensorflow:global_step/sec: 1812.38\n",
      "INFO:tensorflow:step = 1701, loss = 0.0427562 (0.055 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.416\n",
      "INFO:tensorflow:step = 1801, loss = 0.0421692 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 1626.57\n",
      "INFO:tensorflow:step = 1901, loss = 0.0416092 (0.062 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/iris_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0410293.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(params={'hidden_units': [10, 20, 10], 'optimizer': None, 'embedding_lr_multipliers': None, 'feature_columns': (_RealValuedColumn(column_name='', dimension=4, default_value=None, dtype=tf.float32, normalizer=None),), 'input_layer_min_slice_size': None, 'activation_fn': <function relu at 0x7f63ebe08268>, 'gradient_clip_norm': None, 'dropout': None, 'head': <tensorflow.contrib.learn.python.learn.estimators.head._MultiClassHead object at 0x7f632fba5278>})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(input_fn=get_train_inputs, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.4/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-06-19-15:23:18\n",
      "INFO:tensorflow:Restoring parameters from /tmp/iris_model/model.ckpt-2000\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-06-19-15:23:18\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.966667, global_step = 2000, loss = 0.060426\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "\n",
      "Test Accuracy: 0.966667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the test inputs\n",
    "def get_test_inputs():\n",
    "  x = tf.constant(test_set.data)\n",
    "  y = tf.constant(test_set.target)\n",
    "\n",
    "  return x, y\n",
    "\n",
    "# Evaluate accuracy.\n",
    "accuracy_score = classifier.evaluate(input_fn=get_test_inputs,\n",
    "                                     steps=1)[\"accuracy\"]\n",
    "\n",
    "print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.4/dist-packages/tensorflow/python/util/deprecation.py:335: calling DNNClassifier.predict (from tensorflow.contrib.learn.python.learn.estimators.dnn) with outputs=None is deprecated and will be removed after 2017-03-01.\n",
      "Instructions for updating:\n",
      "Please switch to predict_classes, or set `outputs` argument.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/iris_model/model.ckpt-2000\n",
      "New Samples, Class Predictions:    [1, 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classify two new flower samples.\n",
    "def new_samples():\n",
    "  return np.array(\n",
    "    [[6.4, 3.2, 4.5, 1.5],\n",
    "     [5.8, 3.1, 5.0, 1.7]], dtype=np.float32)\n",
    "\n",
    "predictions = list(classifier.predict(input_fn=new_samples))\n",
    "\n",
    "print(\n",
    "    \"New Samples, Class Predictions:    {}\\n\"\n",
    "    .format(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Tanh_30:0\", shape=(5, 4), dtype=float32)\n",
      "Tensor(\"Tanh_31:0\", shape=(5, 4), dtype=float32)\n",
      "Tensor(\"Tanh_32:0\", shape=(5, 4), dtype=float32)\n",
      "Tensor(\"Tanh_33:0\", shape=(5, 4), dtype=float32)\n",
      "Tensor(\"Tanh_34:0\", shape=(5, 4), dtype=float32)\n",
      "Tensor(\"Tanh_35:0\", shape=(5, 4), dtype=float32)\n",
      "Tensor(\"Tanh_36:0\", shape=(5, 4), dtype=float32)\n",
      "Tensor(\"Tanh_37:0\", shape=(5, 4), dtype=float32)\n",
      "Tensor(\"Tanh_38:0\", shape=(5, 4), dtype=float32)\n",
      "Tensor(\"Tanh_39:0\", shape=(5, 4), dtype=float32)\n",
      "Tensor(\"Tanh_40:0\", shape=(5, 4), dtype=float32)\n",
      "Tensor(\"Tanh_41:0\", shape=(5, 4), dtype=float32)\n",
      "Tensor(\"Tanh_42:0\", shape=(5, 4), dtype=float32)\n",
      "Tensor(\"Tanh_43:0\", shape=(5, 4), dtype=float32)\n",
      "Tensor(\"Tanh_44:0\", shape=(5, 4), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-7-3ddfea0a6943>:84: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f450ff35d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data, epoch 0\n",
      "Step 0 Loss 0.774378\n",
      "Step 100 Loss 0.693082\n",
      "Step 200 Loss 0.693094\n",
      "Step 300 Loss 0.696337\n",
      "Step 400 Loss 0.705557\n",
      "Step 500 Loss 0.693068\n",
      "Step 600 Loss 0.520961\n",
      "New data, epoch 1\n",
      "Step 0 Loss 0.260886\n",
      "Step 100 Loss 0.0198552\n",
      "Step 200 Loss 0.0106842\n",
      "Step 300 Loss 0.00828137\n",
      "Step 400 Loss 0.00476472\n",
      "Step 500 Loss 0.00479359\n",
      "Step 600 Loss 0.00344917\n",
      "New data, epoch 2\n",
      "Step 0 Loss 0.135703\n",
      "Step 100 Loss 0.00285291\n",
      "Step 200 Loss 0.00255162\n",
      "Step 300 Loss 0.00220821\n",
      "Step 400 Loss 0.00215666\n",
      "Step 500 Loss 0.00180039\n",
      "Step 600 Loss 0.00166162\n",
      "New data, epoch 3\n",
      "Step 0 Loss 0.166419\n",
      "Step 100 Loss 0.514644\n",
      "Step 200 Loss 0.420035\n",
      "Step 300 Loss 0.344175\n",
      "Step 400 Loss 0.392095\n",
      "Step 500 Loss 0.282062\n",
      "Step 600 Loss 0.26042\n",
      "New data, epoch 4\n",
      "Step 0 Loss 0.624267\n",
      "Step 100 Loss 0.246581\n",
      "Step 200 Loss 0.416803\n",
      "Step 300 Loss 0.36633\n",
      "Step 400 Loss 0.232608\n",
      "Step 500 Loss 0.226707\n",
      "Step 600 Loss 0.215098\n",
      "New data, epoch 5\n",
      "Step 0 Loss 0.71112\n",
      "Step 100 Loss 0.161435\n",
      "Step 200 Loss 0.41911\n",
      "Step 300 Loss 0.24942\n",
      "Step 400 Loss 0.304165\n",
      "Step 500 Loss 0.381275\n",
      "Step 600 Loss 0.410683\n",
      "New data, epoch 6\n",
      "Step 0 Loss 0.614409\n",
      "Step 100 Loss 0.237892\n",
      "Step 200 Loss 0.242118\n",
      "Step 300 Loss 0.301632\n",
      "Step 400 Loss 0.329597\n",
      "Step 500 Loss 0.0101225\n",
      "Step 600 Loss 0.00623561\n",
      "New data, epoch 7\n",
      "Step 0 Loss 0.145411\n",
      "Step 100 Loss 0.00332955\n",
      "Step 200 Loss 0.00353979\n",
      "Step 300 Loss 0.00190732\n",
      "Step 400 Loss 0.00172111\n",
      "Step 500 Loss 0.0017629\n",
      "Step 600 Loss 0.00155338\n",
      "New data, epoch 8\n",
      "Step 0 Loss 0.143464\n",
      "Step 100 Loss 0.00121581\n",
      "Step 200 Loss 0.00143118\n",
      "Step 300 Loss 0.00123223\n",
      "Step 400 Loss 0.00109548\n",
      "Step 500 Loss 0.00113319\n",
      "Step 600 Loss 0.00110299\n",
      "New data, epoch 9\n",
      "Step 0 Loss 0.192865\n",
      "Step 100 Loss 0.000824463\n",
      "Step 200 Loss 0.000869627\n",
      "Step 300 Loss 0.000807862\n",
      "Step 400 Loss 0.00119978\n",
      "Step 500 Loss 0.000765941\n",
      "Step 600 Loss 0.00112651\n",
      "New data, epoch 10\n",
      "Step 0 Loss 0.228652\n",
      "Step 100 Loss 0.00102915\n",
      "Step 200 Loss 0.000680907\n",
      "Step 300 Loss 0.000848143\n",
      "Step 400 Loss 0.00104743\n",
      "Step 500 Loss 0.000733175\n",
      "Step 600 Loss 0.000805719\n",
      "New data, epoch 11\n",
      "Step 0 Loss 0.122006\n",
      "Step 100 Loss 0.000696465\n",
      "Step 200 Loss 0.000741621\n",
      "Step 300 Loss 0.000687196\n",
      "Step 400 Loss 0.000714628\n",
      "Step 500 Loss 0.000542796\n",
      "Step 600 Loss 0.000613277\n",
      "New data, epoch 12\n",
      "Step 0 Loss 0.13024\n",
      "Step 100 Loss 0.000701866\n",
      "Step 200 Loss 0.000452936\n",
      "Step 300 Loss 0.000790271\n",
      "Step 400 Loss 0.000655572\n",
      "Step 500 Loss 0.000507673\n",
      "Step 600 Loss 0.000553845\n",
      "New data, epoch 13\n",
      "Step 0 Loss 0.16696\n",
      "Step 100 Loss 0.000692205\n",
      "Step 200 Loss 0.000455923\n",
      "Step 300 Loss 0.000680848\n",
      "Step 400 Loss 0.000785275\n",
      "Step 500 Loss 0.000421247\n",
      "Step 600 Loss 0.00053888\n",
      "New data, epoch 14\n",
      "Step 0 Loss 0.165451\n",
      "Step 100 Loss 0.000626116\n",
      "Step 200 Loss 0.000687856\n",
      "Step 300 Loss 0.00060883\n",
      "Step 400 Loss 0.000443926\n",
      "Step 500 Loss 0.000353185\n",
      "Step 600 Loss 0.000413119\n",
      "New data, epoch 15\n",
      "Step 0 Loss 0.172976\n",
      "Step 100 Loss 0.000643439\n",
      "Step 200 Loss 0.000372099\n",
      "Step 300 Loss 0.000473319\n",
      "Step 400 Loss 0.000445357\n",
      "Step 500 Loss 0.000449111\n",
      "Step 600 Loss 0.00042572\n",
      "New data, epoch 16\n",
      "Step 0 Loss 0.232115\n",
      "Step 100 Loss 0.000525571\n",
      "Step 200 Loss 0.000468971\n",
      "Step 300 Loss 0.000516147\n",
      "Step 400 Loss 0.000511475\n",
      "Step 500 Loss 0.000534135\n",
      "Step 600 Loss 0.000522941\n",
      "New data, epoch 17\n",
      "Step 0 Loss 0.234633\n",
      "Step 100 Loss 0.000674112\n",
      "Step 200 Loss 0.000391556\n",
      "Step 300 Loss 0.000371507\n",
      "Step 400 Loss 0.00042525\n",
      "Step 500 Loss 0.000479732\n",
      "Step 600 Loss 0.000523143\n",
      "New data, epoch 18\n",
      "Step 0 Loss 0.110233\n",
      "Step 100 Loss 0.000345475\n",
      "Step 200 Loss 0.000398039\n",
      "Step 300 Loss 0.000464018\n",
      "Step 400 Loss 0.000466298\n",
      "Step 500 Loss 0.000689577\n",
      "Step 600 Loss 0.000368341\n",
      "New data, epoch 19\n",
      "Step 0 Loss 0.151651\n",
      "Step 100 Loss 0.000285839\n",
      "Step 200 Loss 0.000555002\n",
      "Step 300 Loss 0.000515938\n",
      "Step 400 Loss 0.000611107\n",
      "Step 500 Loss 0.00037287\n",
      "Step 600 Loss 0.000328229\n",
      "New data, epoch 20\n",
      "Step 0 Loss 0.163801\n",
      "Step 100 Loss 0.000312686\n",
      "Step 200 Loss 0.000345132\n",
      "Step 300 Loss 0.000323756\n",
      "Step 400 Loss 0.000508187\n",
      "Step 500 Loss 0.000322284\n",
      "Step 600 Loss 0.000352249\n",
      "New data, epoch 21\n",
      "Step 0 Loss 0.166274\n",
      "Step 100 Loss 0.000351127\n",
      "Step 200 Loss 0.000346583\n",
      "Step 300 Loss 0.000365845\n",
      "Step 400 Loss 0.000344306\n",
      "Step 500 Loss 0.000371595\n",
      "Step 600 Loss 0.000266062\n",
      "New data, epoch 22\n",
      "Step 0 Loss 0.0982037\n",
      "Step 100 Loss 0.000332617\n",
      "Step 200 Loss 0.000339635\n",
      "Step 300 Loss 0.000634391\n",
      "Step 400 Loss 0.000333983\n",
      "Step 500 Loss 0.000301209\n",
      "Step 600 Loss 0.000246192\n",
      "New data, epoch 23\n",
      "Step 0 Loss 0.103977\n",
      "Step 100 Loss 0.00042404\n",
      "Step 200 Loss 0.000259944\n",
      "Step 300 Loss 0.000243054\n",
      "Step 400 Loss 0.00026058\n",
      "Step 500 Loss 0.000299714\n",
      "Step 600 Loss 0.000251607\n",
      "New data, epoch 24\n",
      "Step 0 Loss 0.189293\n",
      "Step 100 Loss 0.000681173\n",
      "Step 200 Loss 0.000259472\n",
      "Step 300 Loss 0.000228957\n",
      "Step 400 Loss 0.000330244\n",
      "Step 500 Loss 0.00026883\n",
      "Step 600 Loss 0.000347664\n",
      "New data, epoch 25\n",
      "Step 0 Loss 0.253712\n",
      "Step 100 Loss 0.00056623\n",
      "Step 200 Loss 0.000234826\n",
      "Step 300 Loss 0.000285768\n",
      "Step 400 Loss 0.000450422\n",
      "Step 500 Loss 0.000334758\n",
      "Step 600 Loss 0.000232402\n",
      "New data, epoch 26\n",
      "Step 0 Loss 0.261212\n",
      "Step 100 Loss 0.000421176\n",
      "Step 200 Loss 0.000270853\n",
      "Step 300 Loss 0.000259646\n",
      "Step 400 Loss 0.000252735\n",
      "Step 500 Loss 0.000338684\n",
      "Step 600 Loss 0.000297181\n",
      "New data, epoch 27\n",
      "Step 0 Loss 0.170466\n",
      "Step 100 Loss 0.000346192\n",
      "Step 200 Loss 0.000231473\n",
      "Step 300 Loss 0.000263612\n",
      "Step 400 Loss 0.00022659\n",
      "Step 500 Loss 0.000225068\n",
      "Step 600 Loss 0.000256802\n",
      "New data, epoch 28\n",
      "Step 0 Loss 0.123614\n",
      "Step 100 Loss 0.00032802\n",
      "Step 200 Loss 0.000478322\n",
      "Step 300 Loss 0.000215564\n",
      "Step 400 Loss 0.000241152\n",
      "Step 500 Loss 0.000339673\n",
      "Step 600 Loss 0.000239767\n",
      "New data, epoch 29\n",
      "Step 0 Loss 0.136563\n",
      "Step 100 Loss 0.000292236\n",
      "Step 200 Loss 0.000220631\n",
      "Step 300 Loss 0.00019598\n",
      "Step 400 Loss 0.000313482\n",
      "Step 500 Loss 0.000216837\n",
      "Step 600 Loss 0.000364381\n",
      "New data, epoch 30\n",
      "Step 0 Loss 0.165329\n",
      "Step 100 Loss 0.000177059\n",
      "Step 200 Loss 0.000284389\n",
      "Step 300 Loss 0.000204407\n",
      "Step 400 Loss 0.000265832\n",
      "Step 500 Loss 0.000225806\n",
      "Step 600 Loss 0.000249129\n",
      "New data, epoch 31\n",
      "Step 0 Loss 0.20075\n",
      "Step 100 Loss 0.000252849\n",
      "Step 200 Loss 0.000252142\n",
      "Step 300 Loss 0.000242557\n",
      "Step 400 Loss 0.000218291\n",
      "Step 500 Loss 0.000184478\n",
      "Step 600 Loss 0.000254574\n",
      "New data, epoch 32\n",
      "Step 0 Loss 0.138481\n",
      "Step 100 Loss 0.000232483\n",
      "Step 200 Loss 0.000176936\n",
      "Step 300 Loss 0.000233854\n",
      "Step 400 Loss 0.000324334\n",
      "Step 500 Loss 0.0002829\n",
      "Step 600 Loss 0.000174888\n",
      "New data, epoch 33\n",
      "Step 0 Loss 0.140359\n",
      "Step 100 Loss 0.000226509\n",
      "Step 200 Loss 0.000239691\n",
      "Step 300 Loss 0.000169649\n",
      "Step 400 Loss 0.000240091\n",
      "Step 500 Loss 0.000377202\n",
      "Step 600 Loss 0.000183531\n",
      "New data, epoch 34\n",
      "Step 0 Loss 0.238112\n",
      "Step 100 Loss 0.000244189\n",
      "Step 200 Loss 0.00022944\n",
      "Step 300 Loss 0.000203949\n",
      "Step 400 Loss 0.000253019\n",
      "Step 500 Loss 0.000202051\n",
      "Step 600 Loss 0.000224152\n",
      "New data, epoch 35\n",
      "Step 0 Loss 0.140479\n",
      "Step 100 Loss 0.000182415\n",
      "Step 200 Loss 0.0001753\n",
      "Step 300 Loss 0.000239448\n",
      "Step 400 Loss 0.000255267\n",
      "Step 500 Loss 0.000201353\n",
      "Step 600 Loss 0.0001967\n",
      "New data, epoch 36\n",
      "Step 0 Loss 0.233891\n",
      "Step 100 Loss 0.000268442\n",
      "Step 200 Loss 0.000237917\n",
      "Step 300 Loss 0.000346671\n",
      "Step 400 Loss 0.000236486\n",
      "Step 500 Loss 0.000170702\n",
      "Step 600 Loss 0.000314706\n",
      "New data, epoch 37\n",
      "Step 0 Loss 0.14723\n",
      "Step 100 Loss 0.000166503\n",
      "Step 200 Loss 0.00019674\n",
      "Step 300 Loss 0.000172122\n",
      "Step 400 Loss 0.000193942\n",
      "Step 500 Loss 0.000223848\n",
      "Step 600 Loss 0.000155797\n",
      "New data, epoch 38\n",
      "Step 0 Loss 0.146093\n",
      "Step 100 Loss 0.000259839\n",
      "Step 200 Loss 0.000188676\n",
      "Step 300 Loss 0.000244416\n",
      "Step 400 Loss 0.000150542\n",
      "Step 500 Loss 0.000250969\n",
      "Step 600 Loss 0.000156232\n",
      "New data, epoch 39\n",
      "Step 0 Loss 0.167356\n",
      "Step 100 Loss 0.00029638\n",
      "Step 200 Loss 0.000172824\n",
      "Step 300 Loss 0.000202056\n",
      "Step 400 Loss 0.000186595\n",
      "Step 500 Loss 0.000244686\n",
      "Step 600 Loss 0.000137435\n",
      "New data, epoch 40\n",
      "Step 0 Loss 0.139432\n",
      "Step 100 Loss 0.000191423\n",
      "Step 200 Loss 0.000243619\n",
      "Step 300 Loss 0.000233378\n",
      "Step 400 Loss 0.000187474\n",
      "Step 500 Loss 0.000206573\n",
      "Step 600 Loss 0.000176491\n",
      "New data, epoch 41\n",
      "Step 0 Loss 0.179163\n",
      "Step 100 Loss 0.000246211\n",
      "Step 200 Loss 0.000150476\n",
      "Step 300 Loss 0.000182086\n",
      "Step 400 Loss 0.000165895\n",
      "Step 500 Loss 0.00014696\n",
      "Step 600 Loss 0.0001302\n",
      "New data, epoch 42\n",
      "Step 0 Loss 0.153281\n",
      "Step 100 Loss 0.000145529\n",
      "Step 200 Loss 0.000181797\n",
      "Step 300 Loss 0.00012608\n",
      "Step 400 Loss 0.000156509\n",
      "Step 500 Loss 0.000135526\n",
      "Step 600 Loss 0.000215458\n",
      "New data, epoch 43\n",
      "Step 0 Loss 0.145931\n",
      "Step 100 Loss 0.000221724\n",
      "Step 200 Loss 0.000167649\n",
      "Step 300 Loss 0.000132415\n",
      "Step 400 Loss 0.000151308\n",
      "Step 500 Loss 0.000146275\n",
      "Step 600 Loss 0.000158967\n",
      "New data, epoch 44\n",
      "Step 0 Loss 0.184246\n",
      "Step 100 Loss 0.000279687\n",
      "Step 200 Loss 0.000207314\n",
      "Step 300 Loss 0.000265183\n",
      "Step 400 Loss 0.000173119\n",
      "Step 500 Loss 0.000160146\n",
      "Step 600 Loss 0.000106937\n",
      "New data, epoch 45\n",
      "Step 0 Loss 0.121612\n",
      "Step 100 Loss 0.000293418\n",
      "Step 200 Loss 0.000181873\n",
      "Step 300 Loss 0.000140978\n",
      "Step 400 Loss 0.000152934\n",
      "Step 500 Loss 0.000250423\n",
      "Step 600 Loss 0.000148188\n",
      "New data, epoch 46\n",
      "Step 0 Loss 0.115852\n",
      "Step 100 Loss 0.000117514\n",
      "Step 200 Loss 0.00012735\n",
      "Step 300 Loss 0.000232223\n",
      "Step 400 Loss 0.000120466\n",
      "Step 500 Loss 0.000231904\n",
      "Step 600 Loss 0.000151473\n",
      "New data, epoch 47\n",
      "Step 0 Loss 0.248073\n",
      "Step 100 Loss 0.00027866\n",
      "Step 200 Loss 0.000137351\n",
      "Step 300 Loss 0.000227362\n",
      "Step 400 Loss 0.000153595\n",
      "Step 500 Loss 0.000142923\n",
      "Step 600 Loss 0.000194119\n",
      "New data, epoch 48\n",
      "Step 0 Loss 0.143435\n",
      "Step 100 Loss 0.000169291\n",
      "Step 200 Loss 0.000140656\n",
      "Step 300 Loss 0.000154806\n",
      "Step 400 Loss 0.000211992\n",
      "Step 500 Loss 0.000153839\n",
      "Step 600 Loss 0.00016695\n",
      "New data, epoch 49\n",
      "Step 0 Loss 0.147514\n",
      "Step 100 Loss 0.000136136\n",
      "Step 200 Loss 0.000161268\n",
      "Step 300 Loss 0.000200422\n",
      "Step 400 Loss 0.000134646\n",
      "Step 500 Loss 0.00011993\n",
      "Step 600 Loss 0.000112151\n",
      "New data, epoch 50\n",
      "Step 0 Loss 0.215982\n",
      "Step 100 Loss 0.000133025\n",
      "Step 200 Loss 0.000132403\n",
      "Step 300 Loss 0.000126546\n",
      "Step 400 Loss 0.000131067\n",
      "Step 500 Loss 0.000114099\n",
      "Step 600 Loss 0.000111436\n",
      "New data, epoch 51\n",
      "Step 0 Loss 0.155049\n",
      "Step 100 Loss 0.000103405\n",
      "Step 200 Loss 0.00012456\n",
      "Step 300 Loss 0.000112699\n",
      "Step 400 Loss 0.000107315\n",
      "Step 500 Loss 0.000101854\n",
      "Step 600 Loss 0.000145532\n",
      "New data, epoch 52\n",
      "Step 0 Loss 0.115043\n",
      "Step 100 Loss 0.000138816\n",
      "Step 200 Loss 0.000116158\n",
      "Step 300 Loss 0.000109394\n",
      "Step 400 Loss 8.97907e-05\n",
      "Step 500 Loss 0.000109003\n",
      "Step 600 Loss 0.000124622\n",
      "New data, epoch 53\n",
      "Step 0 Loss 0.132983\n",
      "Step 100 Loss 0.000163833\n",
      "Step 200 Loss 0.000124342\n",
      "Step 300 Loss 0.000114071\n",
      "Step 400 Loss 0.000151988\n",
      "Step 500 Loss 0.000130841\n",
      "Step 600 Loss 0.000106493\n",
      "New data, epoch 54\n",
      "Step 0 Loss 0.123286\n",
      "Step 100 Loss 0.000134217\n",
      "Step 200 Loss 9.49019e-05\n",
      "Step 300 Loss 0.000195349\n",
      "Step 400 Loss 9.60197e-05\n",
      "Step 500 Loss 9.78498e-05\n",
      "Step 600 Loss 0.000113705\n",
      "New data, epoch 55\n",
      "Step 0 Loss 0.158911\n",
      "Step 100 Loss 0.000101141\n",
      "Step 200 Loss 9.50212e-05\n",
      "Step 300 Loss 0.000121345\n",
      "Step 400 Loss 0.000105577\n",
      "Step 500 Loss 0.000161764\n",
      "Step 600 Loss 0.000129396\n",
      "New data, epoch 56\n",
      "Step 0 Loss 0.107703\n",
      "Step 100 Loss 0.000132268\n",
      "Step 200 Loss 0.000132307\n",
      "Step 300 Loss 0.000105714\n",
      "Step 400 Loss 0.000115138\n",
      "Step 500 Loss 0.000108061\n",
      "Step 600 Loss 9.49365e-05\n",
      "New data, epoch 57\n",
      "Step 0 Loss 0.127595\n",
      "Step 100 Loss 0.000115842\n",
      "Step 200 Loss 0.000117755\n",
      "Step 300 Loss 0.000127466\n",
      "Step 400 Loss 0.000102229\n",
      "Step 500 Loss 0.000111718\n",
      "Step 600 Loss 8.28229e-05\n",
      "New data, epoch 58\n",
      "Step 0 Loss 0.125682\n",
      "Step 100 Loss 8.26437e-05\n",
      "Step 200 Loss 8.78248e-05\n",
      "Step 300 Loss 8.82122e-05\n",
      "Step 400 Loss 0.000113394\n",
      "Step 500 Loss 0.00014396\n",
      "Step 600 Loss 0.000175326\n",
      "New data, epoch 59\n",
      "Step 0 Loss 0.16002\n",
      "Step 100 Loss 8.47666e-05\n",
      "Step 200 Loss 9.9616e-05\n",
      "Step 300 Loss 0.000102249\n",
      "Step 400 Loss 8.82144e-05\n",
      "Step 500 Loss 9.27245e-05\n",
      "Step 600 Loss 0.000120459\n",
      "New data, epoch 60\n",
      "Step 0 Loss 0.238166\n",
      "Step 100 Loss 0.000146009\n",
      "Step 200 Loss 0.000131371\n",
      "Step 300 Loss 0.000124409\n",
      "Step 400 Loss 9.18909e-05\n",
      "Step 500 Loss 0.000132389\n",
      "Step 600 Loss 8.66898e-05\n",
      "New data, epoch 61\n",
      "Step 0 Loss 0.209577\n",
      "Step 100 Loss 9.3956e-05\n",
      "Step 200 Loss 0.000103155\n",
      "Step 300 Loss 0.000115109\n",
      "Step 400 Loss 9.82618e-05\n",
      "Step 500 Loss 0.000133179\n",
      "Step 600 Loss 7.87834e-05\n",
      "New data, epoch 62\n",
      "Step 0 Loss 0.12604\n",
      "Step 100 Loss 7.21193e-05\n",
      "Step 200 Loss 8.64058e-05\n",
      "Step 300 Loss 8.22745e-05\n",
      "Step 400 Loss 7.60925e-05\n",
      "Step 500 Loss 9.06936e-05\n",
      "Step 600 Loss 0.000100486\n",
      "New data, epoch 63\n",
      "Step 0 Loss 0.162147\n",
      "Step 100 Loss 0.000131285\n",
      "Step 200 Loss 9.78176e-05\n",
      "Step 300 Loss 0.000107124\n",
      "Step 400 Loss 0.000104588\n",
      "Step 500 Loss 7.8639e-05\n",
      "Step 600 Loss 7.83394e-05\n",
      "New data, epoch 64\n",
      "Step 0 Loss 0.172225\n",
      "Step 100 Loss 7.09854e-05\n",
      "Step 200 Loss 0.000116446\n",
      "Step 300 Loss 9.44156e-05\n",
      "Step 400 Loss 0.000108722\n",
      "Step 500 Loss 7.84599e-05\n",
      "Step 600 Loss 7.91175e-05\n",
      "New data, epoch 65\n",
      "Step 0 Loss 0.138047\n",
      "Step 100 Loss 0.000219381\n",
      "Step 200 Loss 0.000115405\n",
      "Step 300 Loss 8.4961e-05\n",
      "Step 400 Loss 0.000141026\n",
      "Step 500 Loss 8.75026e-05\n",
      "Step 600 Loss 8.96363e-05\n",
      "New data, epoch 66\n",
      "Step 0 Loss 0.176072\n",
      "Step 100 Loss 7.44602e-05\n",
      "Step 200 Loss 0.000128949\n",
      "Step 300 Loss 0.000129963\n",
      "Step 400 Loss 9.31005e-05\n",
      "Step 500 Loss 0.000167848\n",
      "Step 600 Loss 0.000135908\n",
      "New data, epoch 67\n",
      "Step 0 Loss 0.123055\n",
      "Step 100 Loss 8.58784e-05\n",
      "Step 200 Loss 0.00010362\n",
      "Step 300 Loss 0.00013132\n",
      "Step 400 Loss 7.61432e-05\n",
      "Step 500 Loss 8.10169e-05\n",
      "Step 600 Loss 0.000151312\n",
      "New data, epoch 68\n",
      "Step 0 Loss 0.146123\n",
      "Step 100 Loss 0.000128952\n",
      "Step 200 Loss 0.000142202\n",
      "Step 300 Loss 0.000116034\n",
      "Step 400 Loss 8.09819e-05\n",
      "Step 500 Loss 9.0109e-05\n",
      "Step 600 Loss 8.14268e-05\n",
      "New data, epoch 69\n",
      "Step 0 Loss 0.131951\n",
      "Step 100 Loss 0.000100782\n",
      "Step 200 Loss 8.6062e-05\n",
      "Step 300 Loss 9.60982e-05\n",
      "Step 400 Loss 8.15887e-05\n",
      "Step 500 Loss 8.4322e-05\n",
      "Step 600 Loss 9.55351e-05\n",
      "New data, epoch 70\n",
      "Step 0 Loss 0.133857\n",
      "Step 100 Loss 0.000112024\n",
      "Step 200 Loss 7.75929e-05\n",
      "Step 300 Loss 9.69481e-05\n",
      "Step 400 Loss 7.44431e-05\n",
      "Step 500 Loss 7.12066e-05\n",
      "Step 600 Loss 8.0419e-05\n",
      "New data, epoch 71\n",
      "Step 0 Loss 0.138333\n",
      "Step 100 Loss 8.46968e-05\n",
      "Step 200 Loss 0.000130626\n",
      "Step 300 Loss 0.000102721\n",
      "Step 400 Loss 0.000103757\n",
      "Step 500 Loss 0.000130262\n",
      "Step 600 Loss 7.43659e-05\n",
      "New data, epoch 72\n",
      "Step 0 Loss 0.244508\n",
      "Step 100 Loss 8.9119e-05\n",
      "Step 200 Loss 8.03842e-05\n",
      "Step 300 Loss 7.65325e-05\n",
      "Step 400 Loss 6.89059e-05\n",
      "Step 500 Loss 0.000145227\n",
      "Step 600 Loss 6.4965e-05\n",
      "New data, epoch 73\n",
      "Step 0 Loss 0.163487\n",
      "Step 100 Loss 0.000115536\n",
      "Step 200 Loss 7.86309e-05\n",
      "Step 300 Loss 6.28323e-05\n",
      "Step 400 Loss 7.80052e-05\n",
      "Step 500 Loss 7.00921e-05\n",
      "Step 600 Loss 0.000156048\n",
      "New data, epoch 74\n",
      "Step 0 Loss 0.150525\n",
      "Step 100 Loss 6.88639e-05\n",
      "Step 200 Loss 6.94618e-05\n",
      "Step 300 Loss 0.000105534\n",
      "Step 400 Loss 6.30849e-05\n",
      "Step 500 Loss 9.1011e-05\n",
      "Step 600 Loss 6.01487e-05\n",
      "New data, epoch 75\n",
      "Step 0 Loss 0.171159\n",
      "Step 100 Loss 0.000110637\n",
      "Step 200 Loss 0.000120533\n",
      "Step 300 Loss 0.000141338\n",
      "Step 400 Loss 7.5177e-05\n",
      "Step 500 Loss 7.82794e-05\n",
      "Step 600 Loss 6.70485e-05\n",
      "New data, epoch 76\n",
      "Step 0 Loss 0.168568\n",
      "Step 100 Loss 8.71379e-05\n",
      "Step 200 Loss 7.58466e-05\n",
      "Step 300 Loss 0.000136626\n",
      "Step 400 Loss 9.24613e-05\n",
      "Step 500 Loss 8.41573e-05\n",
      "Step 600 Loss 7.52296e-05\n",
      "New data, epoch 77\n",
      "Step 0 Loss 0.161441\n",
      "Step 100 Loss 0.000141923\n",
      "Step 200 Loss 7.49635e-05\n",
      "Step 300 Loss 8.15613e-05\n",
      "Step 400 Loss 6.75524e-05\n",
      "Step 500 Loss 6.36735e-05\n",
      "Step 600 Loss 6.51911e-05\n",
      "New data, epoch 78\n",
      "Step 0 Loss 0.255801\n",
      "Step 100 Loss 0.000100165\n",
      "Step 200 Loss 0.000111218\n",
      "Step 300 Loss 8.09382e-05\n",
      "Step 400 Loss 0.000129399\n",
      "Step 500 Loss 8.71082e-05\n",
      "Step 600 Loss 6.51598e-05\n",
      "New data, epoch 79\n",
      "Step 0 Loss 0.155371\n",
      "Step 100 Loss 6.97888e-05\n",
      "Step 200 Loss 9.66843e-05\n",
      "Step 300 Loss 0.000118545\n",
      "Step 400 Loss 8.29175e-05\n",
      "Step 500 Loss 7.94381e-05\n",
      "Step 600 Loss 8.24458e-05\n",
      "New data, epoch 80\n",
      "Step 0 Loss 0.14625\n",
      "Step 100 Loss 9.7068e-05\n",
      "Step 200 Loss 6.24011e-05\n",
      "Step 300 Loss 0.000104709\n",
      "Step 400 Loss 6.27863e-05\n",
      "Step 500 Loss 9.67066e-05\n",
      "Step 600 Loss 6.26045e-05\n",
      "New data, epoch 81\n",
      "Step 0 Loss 0.11974\n",
      "Step 100 Loss 8.54739e-05\n",
      "Step 200 Loss 6.6734e-05\n",
      "Step 300 Loss 5.70754e-05\n",
      "Step 400 Loss 7.72566e-05\n",
      "Step 500 Loss 0.000114136\n",
      "Step 600 Loss 9.90319e-05\n",
      "New data, epoch 82\n",
      "Step 0 Loss 0.117562\n",
      "Step 100 Loss 8.66972e-05\n",
      "Step 200 Loss 6.2314e-05\n",
      "Step 300 Loss 7.30357e-05\n",
      "Step 400 Loss 6.89355e-05\n",
      "Step 500 Loss 6.36865e-05\n",
      "Step 600 Loss 6.41371e-05\n",
      "New data, epoch 83\n",
      "Step 0 Loss 0.16498\n",
      "Step 100 Loss 7.21514e-05\n",
      "Step 200 Loss 0.000141004\n",
      "Step 300 Loss 9.90967e-05\n",
      "Step 400 Loss 7.8494e-05\n",
      "Step 500 Loss 7.25288e-05\n",
      "Step 600 Loss 0.000295301\n",
      "New data, epoch 84\n",
      "Step 0 Loss 0.187989\n",
      "Step 100 Loss 7.06156e-05\n",
      "Step 200 Loss 8.09063e-05\n",
      "Step 300 Loss 0.000129775\n",
      "Step 400 Loss 0.000121496\n",
      "Step 500 Loss 7.99393e-05\n",
      "Step 600 Loss 0.000102348\n",
      "New data, epoch 85\n",
      "Step 0 Loss 0.138928\n",
      "Step 100 Loss 9.09636e-05\n",
      "Step 200 Loss 0.000126069\n",
      "Step 300 Loss 6.47715e-05\n",
      "Step 400 Loss 0.000101809\n",
      "Step 500 Loss 7.94142e-05\n",
      "Step 600 Loss 7.11758e-05\n",
      "New data, epoch 86\n",
      "Step 0 Loss 0.147228\n",
      "Step 100 Loss 8.1748e-05\n",
      "Step 200 Loss 7.53754e-05\n",
      "Step 300 Loss 6.43635e-05\n",
      "Step 400 Loss 6.52206e-05\n",
      "Step 500 Loss 5.82768e-05\n",
      "Step 600 Loss 7.15561e-05\n",
      "New data, epoch 87\n",
      "Step 0 Loss 0.149089\n",
      "Step 100 Loss 6.98483e-05\n",
      "Step 200 Loss 6.94511e-05\n",
      "Step 300 Loss 5.78465e-05\n",
      "Step 400 Loss 7.26378e-05\n",
      "Step 500 Loss 7.63206e-05\n",
      "Step 600 Loss 5.52603e-05\n",
      "New data, epoch 88\n",
      "Step 0 Loss 0.110157\n",
      "Step 100 Loss 6.59911e-05\n",
      "Step 200 Loss 0.000111406\n",
      "Step 300 Loss 6.53334e-05\n",
      "Step 400 Loss 6.14027e-05\n",
      "Step 500 Loss 6.78895e-05\n",
      "Step 600 Loss 6.36177e-05\n",
      "New data, epoch 89\n",
      "Step 0 Loss 0.113594\n",
      "Step 100 Loss 0.000102387\n",
      "Step 200 Loss 7.39802e-05\n",
      "Step 300 Loss 5.84334e-05\n",
      "Step 400 Loss 6.21296e-05\n",
      "Step 500 Loss 5.23882e-05\n",
      "Step 600 Loss 6.16477e-05\n",
      "New data, epoch 90\n",
      "Step 0 Loss 0.15465\n",
      "Step 100 Loss 7.86968e-05\n",
      "Step 200 Loss 8.94914e-05\n",
      "Step 300 Loss 0.000103482\n",
      "Step 400 Loss 7.38037e-05\n",
      "Step 500 Loss 6.19314e-05\n",
      "Step 600 Loss 7.28476e-05\n",
      "New data, epoch 91\n",
      "Step 0 Loss 0.152145\n",
      "Step 100 Loss 7.56312e-05\n",
      "Step 200 Loss 6.25498e-05\n",
      "Step 300 Loss 5.60269e-05\n",
      "Step 400 Loss 5.49574e-05\n",
      "Step 500 Loss 6.44595e-05\n",
      "Step 600 Loss 7.1098e-05\n",
      "New data, epoch 92\n",
      "Step 0 Loss 0.142786\n",
      "Step 100 Loss 7.74145e-05\n",
      "Step 200 Loss 8.36678e-05\n",
      "Step 300 Loss 8.97086e-05\n",
      "Step 400 Loss 8.23974e-05\n",
      "Step 500 Loss 0.000106631\n",
      "Step 600 Loss 7.74662e-05\n",
      "New data, epoch 93\n",
      "Step 0 Loss 0.122971\n",
      "Step 100 Loss 7.71377e-05\n",
      "Step 200 Loss 8.65893e-05\n",
      "Step 300 Loss 8.22471e-05\n",
      "Step 400 Loss 5.86548e-05\n",
      "Step 500 Loss 9.96418e-05\n",
      "Step 600 Loss 7.58969e-05\n",
      "New data, epoch 94\n",
      "Step 0 Loss 0.239257\n",
      "Step 100 Loss 9.51765e-05\n",
      "Step 200 Loss 8.17835e-05\n",
      "Step 300 Loss 0.000101465\n",
      "Step 400 Loss 6.20969e-05\n",
      "Step 500 Loss 6.21564e-05\n",
      "Step 600 Loss 0.000110972\n",
      "New data, epoch 95\n",
      "Step 0 Loss 0.316488\n",
      "Step 100 Loss 0.000155447\n",
      "Step 200 Loss 5.50844e-05\n",
      "Step 300 Loss 7.40983e-05\n",
      "Step 400 Loss 7.7176e-05\n",
      "Step 500 Loss 6.97765e-05\n",
      "Step 600 Loss 6.86756e-05\n",
      "New data, epoch 96\n",
      "Step 0 Loss 0.213825\n",
      "Step 100 Loss 7.10655e-05\n",
      "Step 200 Loss 7.45378e-05\n",
      "Step 300 Loss 6.48401e-05\n",
      "Step 400 Loss 7.84712e-05\n",
      "Step 500 Loss 5.97769e-05\n",
      "Step 600 Loss 5.66984e-05\n",
      "New data, epoch 97\n",
      "Step 0 Loss 0.215642\n",
      "Step 100 Loss 6.54307e-05\n",
      "Step 200 Loss 7.33055e-05\n",
      "Step 300 Loss 0.00013455\n",
      "Step 400 Loss 7.58477e-05\n",
      "Step 500 Loss 9.36415e-05\n",
      "Step 600 Loss 5.03596e-05\n",
      "New data, epoch 98\n",
      "Step 0 Loss 0.141578\n",
      "Step 100 Loss 5.9572e-05\n",
      "Step 200 Loss 4.9867e-05\n",
      "Step 300 Loss 7.61935e-05\n",
      "Step 400 Loss 6.2849e-05\n",
      "Step 500 Loss 5.52263e-05\n",
      "Step 600 Loss 5.71482e-05\n",
      "New data, epoch 99\n",
      "Step 0 Loss 0.1534\n",
      "Step 100 Loss 6.2009e-05\n",
      "Step 200 Loss 6.8377e-05\n",
      "Step 300 Loss 5.57182e-05\n",
      "Step 400 Loss 8.07571e-05\n",
      "Step 500 Loss 5.41307e-05\n",
      "Step 600 Loss 6.65277e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOBV6EZGM+//MYNpn5bU//wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f44bb5c67f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 100\n",
    "total_series_length = 50000\n",
    "truncated_backprop_length = 15\n",
    "state_size = 4\n",
    "num_classes = 2\n",
    "echo_step = 3\n",
    "batch_size = 5\n",
    "num_batches = total_series_length//batch_size//truncated_backprop_length\n",
    "\n",
    "\n",
    "def generateData():\n",
    "    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n",
    "    y = np.roll(x, echo_step)\n",
    "    y[0:echo_step] = 0\n",
    "\n",
    "    x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n",
    "    y = y.reshape((batch_size, -1))\n",
    "\n",
    "    return (x, y)\n",
    "\n",
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n",
    "\n",
    "init_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "\n",
    "\n",
    "W = tf.Variable(np.random.rand(state_size+1, state_size), dtype=tf.float32)\n",
    "b = tf.Variable(np.zeros((1,state_size)), dtype=tf.float32)\n",
    "\n",
    "W2 = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)\n",
    "\n",
    "inputs_series = tf.unstack(batchX_placeholder, axis=1)\n",
    "labels_series = tf.unstack(batchY_placeholder, axis=1)\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "current_state = init_state\n",
    "states_series = []\n",
    "for current_input in inputs_series:\n",
    "    current_input = tf.reshape(current_input, [batch_size, 1])\n",
    "    input_and_state_concatenated = tf.concat([current_input, current_state], 1)  # Increasing number of columns\n",
    "\n",
    "    next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)  # Broadcasted addition\n",
    "    states_series.append(next_state)\n",
    "    current_state = next_state\n",
    "    print(current_state)\n",
    "\n",
    "logits_series = [tf.matmul(state, W2) + b2 for state in states_series] #Broadcasted addition\n",
    "predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = labels) for logits, labels in zip(logits_series,labels_series)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)\n",
    "\n",
    "\n",
    "def plot(loss_list, predictions_series, batchX, batchY):\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.cla()\n",
    "    plt.plot(loss_list)\n",
    "\n",
    "    for batch_series_idx in range(5):\n",
    "        one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n",
    "        single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n",
    "\n",
    "        plt.subplot(2, 3, batch_series_idx + 2)\n",
    "        plt.cla()\n",
    "        plt.axis([0, truncated_backprop_length, 0, 2])\n",
    "        left_offset = range(truncated_backprop_length)\n",
    "        plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\n",
    "        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n",
    "        plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n",
    "\n",
    "    plt.draw()\n",
    "    plt.pause(0.0001)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    plt.ion()\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        x,y = generateData()\n",
    "        _current_state = np.zeros((batch_size, state_size))\n",
    "\n",
    "        print(\"New data, epoch\", epoch_idx)\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * truncated_backprop_length\n",
    "            end_idx = start_idx + truncated_backprop_length\n",
    "\n",
    "            batchX = x[:,start_idx:end_idx]\n",
    "            batchY = y[:,start_idx:end_idx]\n",
    "\n",
    "            _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n",
    "                [total_loss, train_step, current_state, predictions_series],\n",
    "                feed_dict={\n",
    "                    batchX_placeholder:batchX,\n",
    "                    batchY_placeholder:batchY,\n",
    "                    init_state:_current_state\n",
    "                })\n",
    "\n",
    "            loss_list.append(_total_loss)\n",
    "\n",
    "            if batch_idx%100 == 0:\n",
    "                print(\"Step\",batch_idx, \"Loss\", _total_loss)\n",
    "                plot(loss_list, _predictions_series, batchX, batchY)\n",
    "\n",
    "plt.ioff()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [0, 0, 1, ..., 0, 0, 1],\n",
       "        [1, 1, 0, ..., 1, 1, 1],\n",
       "        [0, 1, 0, ..., 1, 0, 1],\n",
       "        [1, 0, 1, ..., 1, 0, 0]]), array([[0, 0, 0, ..., 0, 0, 1],\n",
       "        [0, 0, 0, ..., 1, 0, 1],\n",
       "        [0, 0, 1, ..., 0, 1, 0],\n",
       "        [1, 1, 1, ..., 0, 1, 0],\n",
       "        [1, 0, 1, ..., 1, 0, 1]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generateData_test():\n",
    "    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n",
    "    y = np.roll(x, echo_step)\n",
    "    y[0:echo_step] = 0\n",
    "    print(len(x))\n",
    "    x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n",
    "    y = y.reshape((batch_size, -1))\n",
    "    print(len(x))\n",
    "    return (x, y)\n",
    "\n",
    "generateData_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.23211504  0.00318773  0.30116192  0.91760795  0.10732129  0.73600577\n",
      "   0.27950449  0.03211105  0.52077141  0.22410896  0.05025499  0.39399029\n",
      "   0.33812705  0.53665485  0.95242824  0.59431162  0.15740781  0.79090562\n",
      "   0.55535617  0.68486821  0.94213726  0.30355274  0.89943826  0.99325804\n",
      "   0.13691922  0.01202364  0.96594092  0.62509675  0.53458163  0.6630219\n",
      "   0.90126068  0.68238189  0.54124585  0.99280968  0.73188868  0.07326856\n",
      "   0.69192502  0.85411392  0.76421806  0.79980548  0.67406094  0.5331657\n",
      "   0.79975372  0.38102414  0.2348      0.73258326  0.67929537  0.20262776\n",
      "   0.48604439  0.37433283  0.68242702  0.54752113  0.07270331  0.53185283\n",
      "   0.17323104  0.64040151  0.42916769  0.46703366  0.3536873   0.70975185\n",
      "   0.55211013  0.09746471  0.57521943  0.1421253 ]]\n"
     ]
    }
   ],
   "source": [
    "matrix = np.random.random([1024, 64])  # 64-dimensional embeddings\n",
    "ids = np.array([0])\n",
    "print(matrix[ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23211504,  0.00318773,  0.30116192, ...,  0.09746471,\n",
       "         0.57521943,  0.1421253 ],\n",
       "       [ 0.94749889,  0.708665  ,  0.64156856, ...,  0.94216069,\n",
       "         0.92006028,  0.69091747],\n",
       "       [ 0.00993109,  0.54933762,  0.25197686, ...,  0.80695523,\n",
       "         0.24085052,  0.0835973 ],\n",
       "       ..., \n",
       "       [ 0.99705169,  0.8037388 ,  0.95313877, ...,  0.85916858,\n",
       "         0.43332443,  0.62091664],\n",
       "       [ 0.96515513,  0.8235149 ,  0.36474146, ...,  0.53279835,\n",
       "         0.90884513,  0.79105174],\n",
       "       [ 0.51547687,  0.50579895,  0.90585121, ...,  0.54374574,\n",
       "         0.14258314,  0.96078944]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def samples_generator(fn, shape, rng, seed):\n",
    "    '''\n",
    "    Generate random samples for the model:\n",
    "    @fn - function to be applied on the input features to get the ouput\n",
    "    @shape - shape of the features matrix (num_samples, num_features)\n",
    "    @rng - range of the input features to be generated within (a,b)\n",
    "    Outputs a tuple of input and output features matrix\n",
    "    '''\n",
    "    prng = RandomState(seed)\n",
    "    x = (rng[1] - rng[0]) * prng.random_sample(shape) + rng[0]\n",
    "    y = np.apply_along_axis(fn, 1, x).reshape((shape[0],-1))\n",
    "    z = np.zeros((shape[0],shape[1] - y.shape[1]))\n",
    "    y = np.concatenate((y, z), axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ -0.99494284, -11.14453818,   6.39338322],\n",
       "        [ 78.43627378,  45.82283344, -17.71479208],\n",
       "        [ 66.15148427,  84.29241089,  57.43980381],\n",
       "        [-72.83165547,  66.2204489 , -34.17141444],\n",
       "        [-57.82161142, -51.78805836, -88.74713915],\n",
       "        [ 66.6172639 ,  80.19252411,   2.2838143 ],\n",
       "        [-74.99486975,  10.44286241,   3.22274634],\n",
       "        [ 11.26747078,  -1.59742759,  79.42957849],\n",
       "        [ 57.8560798 ,  74.78851786, -16.4421817 ],\n",
       "        [ 72.13545842, -31.46897261, -47.77349297]]),\n",
       " array([[  -5.7460978 ,    0.        ,    0.        ],\n",
       "        [ 106.54431513,    0.        ,    0.        ],\n",
       "        [ 207.88369897,    0.        ,    0.        ],\n",
       "        [ -40.78262101,    0.        ,    0.        ],\n",
       "        [-198.35680893,    0.        ,    0.        ],\n",
       "        [ 149.0936023 ,    0.        ,    0.        ],\n",
       "        [ -61.329261  ,    0.        ,    0.        ],\n",
       "        [  89.09962169,    0.        ,    0.        ],\n",
       "        [ 116.20241596,    0.        ,    0.        ],\n",
       "        [  -7.10700716,    0.        ,    0.        ]]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def np_add(vec):\n",
    "    return reduce((lambda x, y: x + y),vec)\n",
    "\n",
    "def np_mult(vec):\n",
    "    return reduce((lambda x, y: x * y),vec)\n",
    "\n",
    "def np_stall(vec):\n",
    "    return vec\n",
    "\n",
    "samples_generator(np_add, (10, 3) , (-100, 100), 45124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_train_test (x, y , test_ratio):\n",
    "    \n",
    "    if y.shape != x.shape:\n",
    "        raise Exception('Model expects x and y shapes to be the same')\n",
    "    \n",
    "    test_len  = int(x.shape[0]*test_ratio)\n",
    "    train_len = x.shape[0] - test_len\n",
    "    \n",
    "    y_train = y[0:train_len][:]\n",
    "    y_test  = y[-test_len:][:]\n",
    "    x_train = x[0:train_len][:]\n",
    "    x_test  = x[-test_len:][:]\n",
    "    \n",
    "    print(train_len)\n",
    "    print(test_len)\n",
    "\n",
    "    train_shape = (train_len, x.shape[1])\n",
    "    test_shape = (test_len, x.shape[1])\n",
    "    \n",
    "    if y_train.shape != train_shape or x_train.shape != train_shape or x_test.shape != test_shape or y_test.shape != test_shape:\n",
    "        raise Exception('One of the conversion test/train shapes gone wrong')\n",
    "    \n",
    "    return y_train, y_test, x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_train_test(x,y, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 118.51300941,    0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [ 117.33725141,    0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [-166.30777447,    0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [  84.54159496,    0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [ 150.17844783,    0.        ,    0.        ,    0.        ,    0.        ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:5][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  51.239329  ,    0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [-250.12438388,    0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [ 130.02461137,    0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [-243.79235305,    0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [ 170.18193292,    0.        ,    0.        ,    0.        ,    0.        ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[5:][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
