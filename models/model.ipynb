{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 1000000\n",
    "total_series_length = 50000\n",
    "train_sample_size = 10000\n",
    "truncated_backprop_length = 15\n",
    "state_size = 4\n",
    "echo_step = 3\n",
    "num_of_operations = 3\n",
    "#num_batches = total_series_length//batch_size//truncated_backprop_length\n",
    "num_layers = 3\n",
    "num_of_steps = 5\n",
    "num_features = 3\n",
    "num_samples = 10\n",
    "max_num_features = 16\n",
    "max_num_outputs = 16\n",
    "batch_size = num_samples\n",
    "def np_add(vec):\n",
    "    return reduce((lambda x, y: x + y),vec)\n",
    "\n",
    "def samples_generator(fn, shape, rng, seed=None):\n",
    "    '''\n",
    "    Generate random samples for the model:\n",
    "    @fn - function to be applied on the input features to get the ouput\n",
    "    @shape - shape of the features matrix (num_samples, num_features)\n",
    "    @rng - range of the input features to be generated within (a,b)\n",
    "    @seed  - generation seed\n",
    "    Outputs a tuple of input and output features matrix\n",
    "    '''\n",
    "    x = (rng[1] - rng[0]) * np.random.random_sample(shape) + rng[0]\n",
    "    y = np.apply_along_axis(fn, 1, x).reshape((-1,1))\n",
    "\n",
    "    #x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n",
    "    #y = y.reshape((batch_size, -1))\n",
    "\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#x,y = samples_generator(add, (num_samples, num_features) , (0, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep 0\n",
      "timestep 1\n",
      "timestep 2\n",
      "timestep 3\n",
      "timestep 4\n",
      "grads are\n",
      "[<tf.Tensor 'gradients_8/AddN_16:0' shape=(7, 4) dtype=float32>, <tf.Tensor 'gradients_8/AddN_15:0' shape=(4,) dtype=float32>, <tf.Tensor 'gradients_8/AddN_14:0' shape=(4, 3) dtype=float32>, <tf.Tensor 'gradients_8/AddN_12:0' shape=(3,) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "def tf_multiply(inpt):\n",
    "    return tf.reshape( tf.reduce_prod(inpt, axis = 1), [batch_size, -1])\n",
    "\n",
    "def tf_add(inpt):\n",
    "    return  tf.reshape( tf.reduce_sum(inpt, axis = 1), [batch_size, -1])\n",
    "\n",
    "def tf_stall(a):\n",
    "    return a\n",
    "\n",
    "'''\n",
    "def tf_divide(inpt):\n",
    "    return tf.einsum('i,j->ij',inpt)\n",
    "\n",
    "def tf_substract(x,y):\n",
    "    return tf.subtract(x,y)\n",
    "'''\n",
    "\n",
    "'''\n",
    "function_dict = {\n",
    "    1   : tf_multiply,\n",
    "    2   : tf_add,\n",
    "    3   : tf_divide,\n",
    "    4   : tf_substract,\n",
    "    5   : tf_retain    \n",
    "}\n",
    "'''\n",
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, None], name=\"batchX\")\n",
    "batchY_placeholder = tf.placeholder(tf.float32, [batch_size, None], name=\"batchY\")\n",
    "\n",
    "init_state = tf.placeholder(tf.float32, [batch_size, state_size], name=\"init_state\")\n",
    "\n",
    "W = tf.Variable(np.random.rand(state_size+num_features, state_size), dtype=tf.float32, name=\"W\")\n",
    "b = tf.Variable(np.zeros((state_size)), dtype=tf.float32, name=\"b\")\n",
    "\n",
    "W2 = tf.Variable(np.random.rand(state_size, num_of_operations),dtype=tf.float32, name=\"W2\")\n",
    "b2 = tf.Variable(np.zeros((num_of_operations)), dtype=tf.float32, name=\"b2\")\n",
    "\n",
    "dummy_matrix = tf.zeros([batch_size, num_features], tf.float32, name=\"dummy_constant\")\n",
    "\n",
    "#inputs_series = tf.unstack(batchX_placeholder, axis=1)\n",
    "#labels_series = tf.unstack(batchY_placeholder, axis=1)\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "#current_state = init_state\n",
    "#states_series = []\n",
    "\n",
    "#output = tf.reshape(current_input, [batch_size, 1])\n",
    "\n",
    "current_state = init_state\n",
    "\n",
    "output = batchX_placeholder\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for timestep in range(num_of_steps):\n",
    "    print(\"timestep \" + str(timestep))\n",
    "    current_input = output\n",
    "        \n",
    "    \n",
    "    \n",
    "    input_and_state_concatenated = tf.concat([current_input, current_state], 1)  # Increasing number of columns\n",
    "    next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)  # Broadcasted addition\n",
    "    current_state = next_state\n",
    "    \n",
    "    #calculate softmax and produce the mask of operations\n",
    "    logits = tf.matmul(next_state, W2) + b2 #Broadcasted addition\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "    #argmax = tf.argmax(softmax, 1)\n",
    "    '''\n",
    "    print(logits)\n",
    "    print(softmax)\n",
    "    print(argmax)\n",
    "    '''\n",
    "    #perform ops\n",
    "    add   = tf_add(current_input)\n",
    "    mult  = tf_multiply(current_input)\n",
    "    stall = tf_stall(current_input)\n",
    "    #add = tf.reshape( tf.reduce_prod(current_input, axis = 1), [batch_size, -1])\n",
    "    #mult = tf.reshape( tf.reduce_sum(current_input, axis = 1), [batch_size, -1])\n",
    "    #stall = current_input\n",
    "    #values = tf.concat([add, mult, stall], 1)\n",
    "    values = tf.concat([add, mult, stall], 1)\n",
    "    values = tf.cast(values,tf.float32)\n",
    "    #get softmaxes for operations\n",
    "    #add_softmax = tf.slice(softmax, [0,0], [batch_size,1])\n",
    "    #mult_softmax = tf.slice(softmax, [0,1], [batch_size,1])\n",
    "    #stall_softmax = tf.slice(softmax, [0,2], [batch_size,1])\n",
    "    #produce output matrix\n",
    "    #onehot  = tf.one_hot(argmax_dum, num_of_operations)\n",
    "    #stall_width = tf.shape(stall)[1]\n",
    "    #stall_select = tf.slice(onehot, [0,2], [batch_size,1])\n",
    "    #mask_arr = [onehot]\n",
    "    #for i in range(num_features-1):\n",
    "    #    mask_arr.append(stall_select)\n",
    "    #mask = tf.concat(mask_arr, 1)\n",
    "    #argmax = tf.reshape( softmax, [batch_size, -1])\n",
    "    #mask = onehot\n",
    "    #mask = tf.cast(mask, tf.float32)\n",
    "    #mask = tf.cast(mask, tf.bool)\n",
    "    #apply mask\n",
    "    #output = tf.boolean_mask(values,mask)\n",
    "    \n",
    "    #in the train mask = saturated softmax for all ops. in test change it to onehot(hardmax)\n",
    "    add_softmax   = tf.slice(softmax, [0,0], [batch_size,1])\n",
    "    mult_softmax  = tf.slice(softmax, [0,1], [batch_size,1])\n",
    "    stall_softmax = tf.slice(softmax, [0,2], [batch_size,1])\n",
    "\n",
    "    add_width   = tf.shape(add)[1]\n",
    "    mult_width  = tf.shape(mult)[1]\n",
    "    stall_width = tf.shape(stall)[1]\n",
    "\n",
    "\n",
    "    add_final   = add * add_softmax\n",
    "    mult_final  = mult * mult_softmax\n",
    "    stall_final = stall * stall_softmax\n",
    "\n",
    "    ##conact add and mult results with zeros matrix\n",
    "    add_final = tf.concat([add_final, tf.slice(dummy_matrix, [0,0], [batch_size, num_features - add_width])], 1) \n",
    "    mult_final = tf.concat([mult_final, tf.slice(dummy_matrix, [0,0], [batch_size, num_features - mult_width])], 1) \n",
    "\n",
    "    output = add_final + mult_final + stall_final\n",
    "    outputs.append(output)\n",
    "    #output = tf.reshape( tf.reduce_sum(output, axis = 1), [batch_size, -1])\n",
    "    #print(select_add)\n",
    "    #print(select_mult)\n",
    "    #print(select_stall)\n",
    "    \n",
    "    #output\n",
    "    #output = select_add\n",
    "    \n",
    "    '''\n",
    "    output = tf.case({\n",
    "                   tf.equal(argmax, tf.constant([0], tf.int64)): lambda: ,\n",
    "                   tf.equal(argmax, tf.constant([1], tf.int64)): lambda:},\n",
    "                   default= lambda: ,\n",
    "                   exclusive=True)\n",
    "    '''\n",
    "    #\n",
    "\n",
    "    #states_series.append(next_state)\n",
    "    \n",
    "    #print(current_state)\n",
    "\n",
    "#logits_series = [tf.matmul(state, W2) + b2 for state in states_series] #Broadcasted addition\n",
    "#predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "    \n",
    "\n",
    "#losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = labels) for logits, labels in zip(logits_series,labels_series)]\n",
    "\n",
    "math_error = 0.5 * tf.square(tf.subtract(tf.reduce_sum(output, axis = 1) , batchY_placeholder))\n",
    "\n",
    "total_loss = tf.reduce_sum(math_error)\n",
    "\n",
    "tf.trainable_variables()\n",
    "grads = tf.gradients(total_loss, [W,b,W2,b2], name=\"gradients\")\n",
    "train_step = tf.train.AdamOptimizer(0.01).minimize(total_loss)\n",
    "print(\"grads are\")\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 280168 Loss 221449.0        4 Loss 8.42918e+10      806 Loss 1.39214e+11      Loss 3.81159e+10               1216 Loss 8.73613e+10      1525 Loss 5.80207e+11           1936 Loss 3.90392e+11     2041 Loss 8.73983e+10           Loss 1.27969e+11     2653 Loss 2.72823e+11     1.07781e+11      2859 Loss 3.69517e+11      3162 Loss 2.09806e+10      3261 Loss 1.7157e+11      3463 Loss 5.18828e+10      3552 Loss 1.45538e+11      3721 Loss 2.41888e+11      3989 Loss 3.41102e+11      4134 Loss 9.06043e+10     Loss 3.52641e+11         4747 Loss 5.41074e+11      4864 Loss 2.97681e+11      5049 Loss 1.66655e+10      5111 Loss 7.42319e+10     5181 Loss 2.27601e+11      Loss 3.43401e+10           5463 Loss 3.76807e+11      5619 Loss 4.87987e+10      Loss 9.80117e+10      Loss 1.28299e+11      8.47771e+10      6174 Loss 5.46042e+11          Loss 4.73605e+10      Loss 6.42243e+10      6651 Loss 3.81387e+11      6742 Loss 6.60392e+11      Loss 5.23832e+10      7023 Loss 1.62921e+11      7122 Loss 1.25634e+11      7218 Loss 1.17497e+11      2.12827e+11          4.26138e+11      Loss 1.76199e+11      7940 Loss 1.72686e+11      7967 Loss 2.91531e+11      8147 Loss 2.30198e+11     8496 Loss 2.67903e+10      8649 Loss 2.62797e+10      5.11299e+10     8968 Loss 1.97078e+11      9030 Loss 9.49734e+10     9200 Loss 1.99129e+11           Loss 1.02538e+11      9602 Loss 5.034e+10          Loss 2.39615e+10          11043 Loss 8.56557e+09      11105 Loss 1.96991e+10      Loss 2.72122e+10     Loss 5.39601e+09      11510 Loss 2.46027e+10      Loss 1.05388e+11     11822 Loss 1.95312e+10      11917 Loss 9.46379e+10      11971 Loss 3.85795e+10      12052 Loss 6.83202e+10           12591 Loss 3.11626e+10      Loss 2.93354e+09          13374 Loss 2.16313e+10      Loss 3.00778e+09      13614 Loss 1.36895e+10      13799 Loss 2.70394e+09      Loss 2.18839e+10      Loss 1.20594e+10      14188 Loss 1.69069e+10      14364 Loss 8.71152e+09     Loss 8.18922e+09      14901 Loss 1.58559e+10          15127 Loss 2.89473e+10      15211 Loss 1.23799e+10     15312 Loss 2.87231e+08      15478 Loss 5.21763e+09          16108 Loss 7.78616e+09               16692 Loss 4.73863e+09      16850 Loss 4.81251e+08      16915 Loss 6.20862e+09     5.90899e+08         Loss 2.59943e+09     17768 Loss 2.00742e+09      17817 Loss 5.90835e+09      18305 Loss 2.81407e+08     18479 Loss 5.51713e+09     Loss 5.26942e+08      18913 Loss 1.16726e+09              8.98564e+08      Loss 9.17615e+08      19632 Loss 1.2853e+09     3.4072e+08      Loss 4.26073e+08      3.59224e+09     20180 Loss 1.90044e+09      1.01491e+09      20500 Loss 9.77625e+07      20617 Loss 1.80743e+09     20712 Loss 2.29254e+08     Loss 8.26066e+08      20879 Loss 3.39722e+08      Loss 4.15383e+08      21121 Loss 1.15379e+09      Loss 3.10186e+08          22137 Loss 2.85229e+08     7.26325e+08     22731 Loss 1.07851e+08          22973 Loss 2.8845e+07     23179 Loss 1.04888e+08      Loss 6.34761e+07      23454 Loss 8.46612e+07      23536 Loss 5.46968e+07      23636 Loss 3.15235e+08      23805 Loss 3.66732e+07      24040 Loss 9.61616e+07      24110 Loss 8.66026e+07     Loss 4.74133e+07      24469 Loss 4.80952e+07      24855 Loss 2.06163e+07      24900 Loss 1.13642e+08      8.02614e+06      25023 Loss 9.46989e+07      25125 Loss 7.49622e+06     Loss 5.79053e+06      25479 Loss 1.6863e+07      25662 Loss 3.88793e+07      26362 Loss 1.74948e+07          Loss 1.72207e+07     7.20575e+06      27047 Loss 1.6447e+07      27155 Loss 3.84654e+06      27213 Loss 1.48173e+06     27460 Loss 1.75284e+06          27940 Loss 1.23668e+07           28428 Loss 2.07049e+06      28657 Loss 4.52473e+06      28724 Loss 862138.0      29016 Loss 2.15795e+06     324994.0     Loss 2.20742e+06     Loss 151972.0      29570 Loss 1.84681e+06      29732 Loss 959045.0      29819 Loss 357579.0      29917 Loss 351526.0     29950 Loss 527882.0     29998 Loss 666822.0      30048 Loss 526086.0      30144 Loss 1.48971e+06     30211 Loss 211526.0      30321 Loss 1.87742e+06      30474 Loss 713157.0     Loss 300374.0      Loss 1.40522e+06      30660 Loss 147751.0     Loss 1.54931e+06      30798 Loss 138982.0      31043 Loss 225309.0      31118 Loss 252318.0      31217 Loss 462663.0           31436 Loss 130640.0      Loss 177012.0               32341 Loss 162932.0      492347.0     Loss 249851.0      32674 Loss 367304.0      33032 Loss 184438.0     90438.0      33289 Loss 158442.0               33672 Loss 235575.0          33840 Loss 327091.0      34121 Loss 121245.0      34198 Loss 219102.0      34254 Loss 157871.0      65627.5     682731.0      34877 Loss 309968.0          34994 Loss 609786.0      160702.0     Loss 83998.3      35619 Loss 110740.0      35678 Loss 176370.0      35745 Loss 157813.0          Loss 95650.5      36028 Loss 104634.0      36135 Loss 107286.0           36330 Loss 259562.0          Loss 183223.0      Loss 101874.0     Loss 207307.0      37195 Loss 985752.0      37280 Loss 493218.0      37359 Loss 189530.0      37531 Loss 214021.0     37621 Loss 236861.0      37756 Loss 167342.0      38072 Loss 2.64547e+06      38200 Loss 480140.0      38255 Loss 539563.0      38330 Loss 529769.0          38694 Loss 254543.0     Loss 642184.0      38987 Loss 662820.0          585923.0      39285 Loss 1.13324e+06      39554 Loss 611959.0     40044 Loss 522018.0      40070 Loss 570505.0      Loss 483450.0      40216 Loss 461543.0     40315 Loss 575654.0      40404 Loss 419984.0      283630.0      40688 Loss 340614.0      40792 Loss 448481.0      40951 Loss 204108.0      41214 Loss 555624.0      41306 Loss 258327.0     337066.0      41505 Loss 242957.0           41736 Loss 325192.0      41898 Loss 221969.0     41976 Loss 207372.0           Loss 149007.0      251548.0          43047 Loss 155605.0      43076 Loss 383020.0      43138 Loss 50345.6      43196 Loss 226225.0              43737 Loss 151853.0      Loss 115897.0      Loss 259629.0     Loss 227537.0     Loss 113517.0           44984 Loss 293018.0      45237 Loss 233384.0      45432 Loss 131322.0      45518 Loss 134418.0      45617 Loss 60858.5      45819 Loss 40316.1      45918 Loss 330252.0      46006 Loss 82867.7     Loss 189320.0           80703.7      46655 Loss 192384.0      46750 Loss 394244.0      46834 Loss 165085.0      Loss 133525.0     47118 Loss 71501.8     47279 Loss 365181.0              Loss 231440.0      47835 Loss 141031.0                   48418 Loss 157908.0     48515 Loss 109409.0      Loss 103171.0     Loss 202782.0      159720.0      48893 Loss 211727.0      48967 Loss 92349.3     Loss 263273.0      49280 Loss 480730.0      49370 Loss 786335.0      49507 Loss 806848.0      49748 Loss 764798.0     49856 Loss 740175.0     Loss 1.32266e+06      50306 Loss 575588.0          50513 Loss 934179.0      50681 Loss 775662.0      50753 Loss 391771.0      Loss 810418.0               51247 Loss 318989.0      51306 Loss 670426.0      51407 Loss 400220.0     51445 Loss 281340.0      552379.0      Loss 689743.0      52076 Loss 460590.0      52350 Loss 565743.0      Loss 501638.0     52945 Loss 404110.0      Loss 561975.0     Loss 497862.0      891912.0      Loss 641701.0     53828 Loss 577953.0     53954 Loss 985400.0     54052 Loss 715486.0      54124 Loss 808125.0              54565 Loss 594964.0      54646 Loss 645346.0      55005 Loss 468775.0     55083 Loss 637423.0      55227 Loss 414416.0      55307 Loss 573393.0           55474 Loss 854973.0      55538 Loss 432888.0               Loss 488191.0      Loss 544088.0      Loss 238612.0           56274 Loss 626217.0      Loss 335123.0      56488 Loss 368273.0     56589 Loss 555946.0     56645 Loss 638390.0      57155 Loss 442738.0      57340 Loss 348994.0     Loss 306194.0          57947 Loss 500515.0     58028 Loss 316463.0      58117 Loss 394773.0           58287 Loss 684373.0      58438 Loss 413496.0      58576 Loss 276071.0      840771.0      58943 Loss 669082.0      59271 Loss 824076.0      59373 Loss 569745.0      59599 Loss 595217.0     Loss 444407.0      608907.0     59956 Loss 448930.0      60034 Loss 365868.0      60100 Loss 564945.0           60261 Loss 402802.0     60417 Loss 494370.0      Loss 440443.0          60781 Loss 145780.0      61078 Loss 456638.0      61240 Loss 149426.0      61401 Loss 875594.0      61461 Loss 558331.0     61557 Loss 898258.0      61997 Loss 673637.0          62150 Loss 736188.0      Loss 539333.0      62370 Loss 884334.0      62514 Loss 745490.0      Loss 757802.0      62725 Loss 611131.0      62805 Loss 747519.0      980282.0          63142 Loss 724385.0     Loss 701667.0      63275 Loss 621675.0      63354 Loss 256586.0      63612 Loss 291337.0     63784 Loss 570232.0      325904.0      Loss 367952.0     64233 Loss 188246.0      64296 Loss 133090.0      134341.0     64694 Loss 218150.0     Loss 289959.0      64933 Loss 208797.0      64984 Loss 297331.0          Loss 218083.0      155082.0      65733 Loss 340290.0      Loss 232906.0     65976 Loss 177012.0      66190 Loss 536886.0     Loss 642500.0      Loss 768201.0      Loss 345223.0     681910.0     66907 Loss 668960.0                67477 Loss 835205.0      67597 Loss 804620.0      67669 Loss 608238.0     Loss 607717.0      67909 Loss 421297.0      68005 Loss 638481.0      68076 Loss 383457.0     68225 Loss 548912.0     68304 Loss 453491.0          68627 Loss 230838.0      69165 Loss 426945.0      69324 Loss 591452.0      69425 Loss 805506.0     399716.0     70069 Loss 492022.0      70273 Loss 490795.0      71160 Loss 459078.0          71348 Loss 350134.0     71610 Loss 554147.0          71847 Loss 538175.0      72064 Loss 566602.0      72313 Loss 761815.0      Loss 630067.0      72628 Loss 961487.0      72730 Loss 1.17906e+06      72896 Loss 665715.0     72991 Loss 457266.0          Loss 589637.0     333975.0      73994 Loss 340169.0      910161.0      74378 Loss 613062.0          74530 Loss 516763.0      607892.0      74772 Loss 773518.0     74940 Loss 1.00813e+06           Loss 581177.0      75190 Loss 493881.0          75640 Loss 1.2049e+06      473845.0           Loss 624366.0      76441 Loss 648370.0         77060 Loss 600528.0          77292 Loss 531735.0     77641 Loss 371189.0      77912 Loss 306118.0      77990 Loss 453530.0     643146.0      Loss 387343.0     78366 Loss 959700.0      78615 Loss 667551.0      78691 Loss 362829.0      78759 Loss 573153.0      79084 Loss 189594.0     79265 Loss 576022.0      79319 Loss 467103.0     Loss 448361.0     79476 Loss 308256.0     Loss 213213.0     79731 Loss 276994.0      79897 Loss 593463.0                80150 Loss 259275.0      80234 Loss 622786.0      80487 Loss 543964.0     80565 Loss 475369.0      Loss 710257.0           554279.0     81460 Loss 745693.0      81549 Loss 478190.0     338493.0     81827 Loss 903530.0      82379 Loss 429783.0      82468 Loss 478029.0     Loss 297332.0      82609 Loss 441066.0      82688 Loss 571298.0      82851 Loss 554816.0      652785.0          83333 Loss 454259.0     454447.0      83633 Loss 380413.0      83911 Loss 6.49096e+06      84009 Loss 1.0088e+06     84104 Loss 1.76304e+06      Loss 1.69898e+06          595229.0         85401 Loss 471607.0      262116.0      85684 Loss 189078.0          Loss 161226.0      Loss 397112.0          256079.0      Loss 416177.0      86928 Loss 144409.0      87123 Loss 390574.0     359026.0      87309 Loss 162019.0     87401 Loss 128629.0      376503.0      87597 Loss 238091.0      87882 Loss 146262.0           88174 Loss 429800.0     88565 Loss 62135.9      88767 Loss 52417.1      Loss 421526.0          89158 Loss 240467.0      Loss 283113.0      89353 Loss 341358.0      Loss 317353.0      Loss 335182.0      89955 Loss 320808.0          Loss 348510.0     Loss 325768.0      90741 Loss 200995.0     Loss 37454.9     Loss 345207.0      118770.0      91651 Loss 212346.0      91754 Loss 137332.0      134316.0     Loss 351699.0     92265 Loss 97400.9      Loss 173878.0      95921.7      93665 Loss 213009.0      93882 Loss 239068.0      94090 Loss 110717.0                     Loss 288427.0     Loss 214789.0     95634 Loss 112467.0           95944 Loss 177391.0      96046 Loss 232246.0      96250 Loss 273124.0      96458 Loss 675812.0      96771 Loss 187180.0     96872 Loss 248280.0      96977 Loss 248978.0      Loss 118313.0         97382 Loss 420108.0           97777 Loss 447539.0         98186 Loss 234255.0          Loss 254633.0      98602 Loss 400593.0      98911 Loss 282196.0      99112 Loss 137677.0     99319 Loss 160867.0      99423 Loss 312448.0      Loss 127301.0      99827 Loss 381148.0          100032 Loss 194492.0      100238 Loss 261364.0     100454 Loss 421563.0     100653 Loss 249217.0     101174 Loss 118305.0     101487 Loss 191251.0      101590 Loss 176490.0      101793 Loss 236405.0     101999 Loss 126486.0           395900.0      103195 Loss 530697.0      103496 Loss 272004.0      103599 Loss 314237.0           Loss 270061.0     99530.0           Loss 514319.0      Loss 157050.0      105127 Loss 112437.0      286373.0      105630 Loss 212583.0     105736 Loss 165554.0     105937 Loss 282157.0      Loss 652103.0      Loss 267341.0     106636 Loss 305484.0     Loss 240686.0      107039 Loss 363651.0     Loss 144824.0      107618 Loss 291566.0     107810 Loss 237824.0     219856.0      108110 Loss 180303.0      168172.0      Loss 444908.0           108690 Loss 541973.0      108972 Loss 480145.0     109069 Loss 265443.0           109784 Loss 130231.0      109887 Loss 114465.0      227141.0     110063 Loss 250065.0     Loss 260702.0      Loss 188503.0           Loss 179352.0      110856 Loss 220211.0      Loss 226750.0      111367 Loss 191911.0      Loss 206696.0      111770 Loss 169767.0      112181 Loss 148738.0         112581 Loss 261618.0     112677 Loss 99786.6     Loss 343489.0           113166 Loss 229089.0      113270 Loss 302995.0      Loss 133754.0     113561 Loss 262414.0     113764 Loss 279055.0      113971 Loss 85864.5      114291 Loss 223560.0          114704 Loss 270521.0      115015 Loss 145987.0      Loss 252770.0      115329 Loss 242428.0     115641 Loss 233191.0          115953 Loss 289394.0     116061 Loss 286470.0      Loss 274633.0     71950.8      116480 Loss 332633.0     Loss 191890.0      Loss 220861.0      116782 Loss 76184.0      134686.0      117392 Loss 303294.0      243726.0         Loss 65457.4      89423.3      117889 Loss 286353.0      118186 Loss 339801.0      Loss 232858.0      329978.0     118813 Loss 201319.0      Loss 143576.0     119122 Loss 54318.2      264378.0      Loss 312288.0     119929 Loss 100156.0     Loss 271438.0      120135 Loss 167806.0          120653 Loss 125530.0      120763 Loss 166615.0      120866 Loss 155412.0     121064 Loss 225559.0      Loss 353292.0      137720.0               Loss 193814.0     121764 Loss 207672.0      Loss 170755.0      205620.0      123168 Loss 370367.0           123365 Loss 323528.0      123573 Loss 82283.1      123672 Loss 148166.0     123779 Loss 103633.0      124282 Loss 208907.0      125064 Loss 162027.0      439069.0      125266 Loss 267146.0      Loss 138550.0              309783.0     126695 Loss 199321.0      Loss 70276.7     127001 Loss 226261.0     127904 Loss 165379.0      Loss 108501.0     128417 Loss 267594.0          128703 Loss 209021.0      Loss 113525.0               Loss 192353.0     130037 Loss 245866.0      130146 Loss 180245.0      130248 Loss 200172.0      130352 Loss 97949.1     119439.0      130874 Loss 454949.0      131274 Loss 103420.0      365060.0           116875.0     Loss 313159.0     132495 Loss 158002.0     174211.0           Loss 365587.0      134285 Loss 205425.0      134492 Loss 237548.0          135016 Loss 302419.0      135338 Loss 406164.0     135439 Loss 289918.0      135640 Loss 194774.0      Loss 140429.0      Loss 87311.2      Loss 86364.5           Loss 128583.0     136783 Loss 181260.0         137191 Loss 152968.0     137504 Loss 213211.0     Loss 212879.0      138109 Loss 211623.0      138213 Loss 194440.0      Loss 307802.0      138528 Loss 90206.9     138633 Loss 42859.7      138734 Loss 328928.0      130970.0     139229 Loss 141502.0     139325 Loss 334336.0      Loss 319625.0      139711 Loss 316831.0           140312 Loss 102562.0      140631 Loss 344949.0      140735 Loss 293243.0     140841 Loss 175738.0     141059 Loss 70997.0      141160 Loss 242671.0      Loss 144947.0      Loss 165241.0      141945 Loss 356051.0     Loss 230471.0      142247 Loss 223943.0      142348 Loss 79823.9               142838 Loss 181624.0      142937 Loss 232944.0      143337 Loss 199680.0      143629 Loss 137987.0      Loss 203671.0      144121 Loss 171211.0      144229 Loss 135520.0      144331 Loss 197222.0     144519 Loss 220924.0      338721.0     144903 Loss 179500.0     120949.0     145315 Loss 315839.0      207100.0      145520 Loss 431383.0     145627 Loss 171076.0      145845 Loss 150712.0     145942 Loss 100996.0     142528.0      Loss 221714.0      146247 Loss 291709.0     146448 Loss 314168.0           Loss 147418.0      146851 Loss 227722.0      147239 Loss 172057.0          147928 Loss 58282.6     148028 Loss 265834.0          148343 Loss 316347.0      148448 Loss 175511.0      148554 Loss 213124.0      148654 Loss 108882.0      149065 Loss 223298.0      Loss 253561.0      149482 Loss 318525.0     149582 Loss 151957.0      149680 Loss 265388.0                149984 Loss 154314.0      150088 Loss 271881.0      150609 Loss 102700.0      150716 Loss 224675.0      150818 Loss 211402.0      151027 Loss 110925.0      151124 Loss 65045.0     151416 Loss 264871.0      151515 Loss 262692.0      151615 Loss 366384.0      152019 Loss 61983.5      152118 Loss 345591.0     152316 Loss 173433.0     152630 Loss 205982.0      152736 Loss 112572.0      153247 Loss 231708.0      153350 Loss 279411.0      Loss 149567.0     153555 Loss 429346.0      153847 Loss 118126.0      99095.5      154049 Loss 257757.0      154152 Loss 231456.0      154347 Loss 159879.0      154540 Loss 291025.0      155012 Loss 205972.0      155421 Loss 112068.0           155733 Loss 238039.0     Loss 156816.0     Loss 227027.0      146208.0      156143 Loss 153895.0     156246 Loss 263236.0     156339 Loss 181732.0     156847 Loss 247273.0      156949 Loss 387268.0      Loss 190955.0     157152 Loss 194192.0      157354 Loss 90877.7     94725.7     157857 Loss 151400.0     Loss 188346.0     327050.0      Loss 225234.0      158854 Loss 318686.0     152928.0     Loss 395358.0      Loss 241215.0      159780 Loss 258106.0      174712.0      160060 Loss 75824.9           160766 Loss 360432.0      160869 Loss 166876.0           Loss 245981.0      161279 Loss 315496.0     Loss 231056.0      161690 Loss 212319.0      161792 Loss 127105.0      161896 Loss 186794.0     161996 Loss 253062.0           162297 Loss 216650.0      162396 Loss 291068.0      162705 Loss 193910.0      163015 Loss 245593.0      164020 Loss 156427.0      164121 Loss 328721.0          164808 Loss 112083.0      164996 Loss 76511.4      155095.0      165692 Loss 100579.0      165902 Loss 292402.0          157936.0     166427 Loss 322186.0     166844 Loss 319577.0      167050 Loss 185324.0      167155 Loss 376165.0     167258 Loss 238476.0      167458 Loss 100692.0          167764 Loss 261045.0      167958 Loss 290988.0      Loss 140268.0      168361 Loss 198151.0      168562 Loss 66032.4     Loss 136575.0      Loss 189254.0      168956 Loss 134750.0      169056 Loss 222459.0      140442.0      169260 Loss 359623.0     169361 Loss 167514.0     169858 Loss 72208.3      164020.0           170713 Loss 340832.0      170817 Loss 350220.0      171022 Loss 256671.0     171228 Loss 188598.0     Loss 212579.0     192082.0     Loss 348075.0      171753 Loss 81209.2     171963 Loss 164749.0      172068 Loss 234291.0      172272 Loss 91576.6      172577 Loss 340391.0      172680 Loss 308943.0      Loss 169298.0     173178 Loss 242629.0      173574 Loss 108367.0           Loss 71570.3     Loss 52604.8      Loss 259526.0      174992 Loss 219728.0           175504 Loss 130965.0     175610 Loss 188348.0     175710 Loss 307748.0      176006 Loss 169437.0     176110 Loss 165016.0      176215 Loss 202781.0     176836 Loss 126740.0     177036 Loss 165547.0      261054.0     Loss 254784.0          197866.0      177956 Loss 288579.0          178370 Loss 208285.0      178474 Loss 126027.0           178675 Loss 370717.0      178982 Loss 223198.0      447387.0          179605 Loss 173273.0      179810 Loss 179742.0      180220 Loss 184438.0     180322 Loss 234506.0     180425 Loss 149174.0          117228.0      Loss 250737.0      181154 Loss 189099.0      181360 Loss 328776.0      181459 Loss 52531.9     181561 Loss 210370.0     181672 Loss 162540.0      181774 Loss 190855.0      70856.0      182081 Loss 97635.8      182181 Loss 145539.0      182281 Loss 211757.0      182384 Loss 198155.0     182487 Loss 291356.0      182586 Loss 158926.0      182687 Loss 165491.0      182795 Loss 388126.0      182897 Loss 358917.0      182999 Loss 150312.0      183095 Loss 130826.0      Loss 164310.0      183503 Loss 147286.0      Loss 193862.0      Loss 142672.0     205095.0     249686.0      184341 Loss 210605.0     184441 Loss 269483.0      184746 Loss 196282.0      184850 Loss 332421.0      184958 Loss 104179.0      185062 Loss 186000.0     Loss 55615.2     185374 Loss 300316.0           185578 Loss 77877.0      186321 Loss 330637.0      Loss 314338.0      186730 Loss 277151.0          186922 Loss 127572.0      Loss 152552.0     197946.0      187436 Loss 170966.0      187536 Loss 226953.0      239591.0      Loss 308118.0      Loss 221690.0      188449 Loss 254555.0      219394.0           189060 Loss 205363.0     189162 Loss 139653.0      189268 Loss 202348.0      189490 Loss 43885.8      189595 Loss 169975.0     189699 Loss 107387.0      92902.9     Loss 116311.0           190420 Loss 241586.0      Loss 209652.0     296114.0      190719 Loss 359694.0      190819 Loss 158850.0      191241 Loss 92859.8     228504.0      192087 Loss 110921.0         Loss 147193.0      Loss 89783.7      192510 Loss 232016.0      192816 Loss 144499.0      192923 Loss 102692.0      193029 Loss 350556.0     193131 Loss 315041.0     193441 Loss 168460.0      274448.0      193747 Loss 192630.0      194154 Loss 73948.0      194257 Loss 238790.0      194561 Loss 173717.0      194661 Loss 155202.0      195179 Loss 64411.7     Loss 254352.0         195691 Loss 289566.0      Loss 179727.0      196629 Loss 215454.0      196940 Loss 147140.0           239338.0      197554 Loss 193510.0      Loss 96382.2      197966 Loss 217510.0      198069 Loss 187282.0      198279 Loss 304839.0     198480 Loss 224850.0      198578 Loss 216624.0      198684 Loss 62508.8      198881 Loss 302052.0     198975 Loss 136094.0      199365 Loss 157229.0      Loss 123126.0      199940 Loss 140903.0          Loss 109859.0     Loss 368221.0     200635 Loss 289289.0     200730 Loss 516111.0     201023 Loss 128431.0     201117 Loss 171991.0     Loss 136227.0      201401 Loss 138563.0      201596 Loss 78665.5     201690 Loss 200821.0      201880 Loss 81059.4      210225.0     202066 Loss 121900.0      202438 Loss 165917.0      Loss 193186.0      Loss 196379.0      202986 Loss 165900.0      203087 Loss 181311.0      203287 Loss 344553.0          203772 Loss 191339.0     204073 Loss 201398.0     204171 Loss 228602.0      Loss 79023.9           Loss 128075.0     204780 Loss 362516.0      Loss 325550.0      205080 Loss 317033.0      205380 Loss 165984.0     103232.0      205681 Loss 228016.0           205992 Loss 353456.0      Loss 145500.0           Loss 277439.0         Loss 205316.0      207102 Loss 406150.0      207290 Loss 208309.0     207393 Loss 115083.0      207496 Loss 237883.0      207595 Loss 146095.0     Loss 341762.0      Loss 214339.0           208367 Loss 256627.0     208560 Loss 147703.0      Loss 208217.0     209034 Loss 181921.0      Loss 196844.0      Loss 319735.0      209907 Loss 474146.0      210097 Loss 417299.0      Loss 371204.0      210691 Loss 58322.3     210795 Loss 147554.0     Loss 308614.0     211109 Loss 139753.0      211321 Loss 99314.1     211427 Loss 85182.4      211733 Loss 238460.0      Loss 52504.9          212054 Loss 399905.0           212365 Loss 162958.0      212566 Loss 399645.0      212977 Loss 335644.0      213176 Loss 173853.0     213277 Loss 141847.0      Loss 111981.0      213776 Loss 246725.0     213877 Loss 261379.0     214079 Loss 29737.8      Loss 344204.0          214589 Loss 288055.0          214793 Loss 135490.0     Loss 313977.0     Loss 214632.0     249159.0     Loss 158381.0           215784 Loss 151116.0      Loss 131525.0     216388 Loss 266208.0      216770 Loss 104594.0      216870 Loss 334344.0     Loss 124656.0           Loss 356529.0           217690 Loss 232277.0      217789 Loss 257116.0      217999 Loss 193082.0      218199 Loss 330184.0      218395 Loss 247630.0      156475.0      Loss 126238.0      219187 Loss 368860.0      219292 Loss 214106.0     219496 Loss 368785.0      219599 Loss 203018.0      Loss 283769.0      219997 Loss 126390.0      220298 Loss 223834.0     220504 Loss 120134.0      Loss 211501.0      Loss 362469.0      221119 Loss 355986.0     Loss 109475.0      221324 Loss 82428.7      Loss 209779.0      221805 Loss 295996.0      222397 Loss 222171.0      222496 Loss 326638.0      222592 Loss 190722.0      Loss 422184.0      222981 Loss 208191.0      187029.0      223475 Loss 147354.0      223575 Loss 291726.0      223677 Loss 143670.0      223978 Loss 157454.0     224079 Loss 294297.0      224183 Loss 331466.0     224282 Loss 93824.7      224380 Loss 448614.0      224478 Loss 244256.0     224576 Loss 88777.1     Loss 378077.0      Loss 118512.0     225270 Loss 185731.0      225371 Loss 255963.0      225083.0      225994 Loss 112356.0     226092 Loss 326785.0      226196 Loss 118945.0      226400 Loss 295349.0      226605 Loss 289103.0      226705 Loss 136115.0      227015 Loss 420963.0      227112 Loss 308215.0      227321 Loss 66843.8      Loss 201496.0     228196 Loss 149737.0      228390 Loss 274501.0      Loss 468042.0              229666 Loss 228947.0     Loss 48933.9     Loss 340994.0      229969 Loss 268201.0      230255 Loss 27769.4      230450 Loss 295794.0      230546 Loss 155011.0     230637 Loss 179355.0      231109 Loss 278708.0      231313 Loss 257249.0               232176 Loss 223616.0      232299 Loss 279891.0      Loss 231010.0     Loss 172340.0      232584 Loss 180576.0          232718 Loss 237165.0      233251 Loss 133654.0      233595 Loss 312506.0      233913 Loss 197205.0     Loss 307186.0      234079 Loss 220987.0      234137 Loss 178658.0      234181 Loss 212842.0         229063.0      234546 Loss 309328.0      147309.0           234999 Loss 216614.0     268470.0          235286 Loss 245580.0               235484 Loss 180841.0      Loss 111791.0           235881 Loss 232258.0      235971 Loss 126740.0          Loss 74454.4      236263 Loss 298809.0     236412 Loss 261850.0      236509 Loss 241877.0     236565 Loss 248543.0          236836 Loss 137265.0      99157.6     237111 Loss 216364.0     237254 Loss 137054.0      89493.7     Loss 110655.0      237745 Loss 361949.0     Loss 146482.0      Loss 267143.0      238021 Loss 297955.0      238105 Loss 249307.0      238206 Loss 179132.0     Loss 160400.0      238578 Loss 181613.0      238805 Loss 304014.0      238860 Loss 165862.0      238938 Loss 326996.0     239114 Loss 134171.0      Loss 195752.0           239387 Loss 87322.2      239636 Loss 394978.0          Loss 80868.1      240150 Loss 126285.0      240236 Loss 245596.0      240494 Loss 259311.0      240655 Loss 362919.0      Loss 165759.0      240808 Loss 238040.0      240970 Loss 191847.0     241194 Loss 204920.0     Loss 281274.0     241346 Loss 338653.0      241482 Loss 190969.0      Loss 264571.0      241708 Loss 220356.0     Loss 139127.0      241841 Loss 157779.0     241914 Loss 132333.0      241992 Loss 203586.0     Loss 137965.0      198050.0           242630 Loss 171244.0      242705 Loss 145887.0      242784 Loss 218878.0      242907 Loss 183055.0      243049 Loss 312367.0          243343 Loss 239155.0     243468 Loss 256652.0     243544 Loss 217489.0      243726 Loss 203199.0      244093 Loss 253621.0      Loss 346966.0     Loss 339514.0          244553 Loss 179920.0      244613 Loss 189848.0      244941 Loss 192160.0      245251 Loss 304636.0      Loss 137124.0      245463 Loss 241690.0      119564.0      245737 Loss 138667.0      245829 Loss 139213.0      Loss 238306.0      Loss 217275.0      246129 Loss 233531.0      246267 Loss 111542.0           246433 Loss 385187.0     246512 Loss 146637.0     Loss 222719.0      246719 Loss 212613.0      246796 Loss 263336.0      247013 Loss 90380.4      247240 Loss 351326.0      247314 Loss 304230.0      Loss 408609.0     Loss 434862.0      247804 Loss 300755.0      89434.0      247971 Loss 111389.0      248039 Loss 328265.0      Loss 179238.0      248312 Loss 106568.0      248464 Loss 252470.0      248603 Loss 168462.0      248711 Loss 148944.0     179493.0      Loss 362948.0      249076 Loss 170934.0      291848.0      249244 Loss 138494.0              Loss 186702.0     249693 Loss 294161.0      250041 Loss 239744.0      250149 Loss 277022.0      250390 Loss 125171.0           250821 Loss 214445.0      Loss 39853.9      251440 Loss 107015.0          299521.0      251892 Loss 227536.0          252145 Loss 96505.3      252219 Loss 151371.0     Loss 200982.0     Loss 173946.0     223652.0     252752 Loss 221665.0      252890 Loss 59995.8     253025 Loss 108704.0      253284 Loss 52363.0     253321 Loss 85024.6      253401 Loss 155225.0      253488 Loss 154197.0     Loss 128045.0      253703 Loss 170177.0      253766 Loss 147099.0      253831 Loss 273121.0      253906 Loss 125698.0      254159 Loss 297608.0           254587 Loss 168353.0      Loss 177334.0      Loss 127774.0      254952 Loss 220996.0          Loss 146610.0      255489 Loss 102116.0      255565 Loss 268425.0     255623 Loss 300530.0      Loss 188513.0      255755 Loss 273191.0      255908 Loss 185511.0      256074 Loss 314575.0      256148 Loss 175288.0          256312 Loss 437912.0      Loss 153963.0      256640 Loss 299551.0      256988 Loss 181945.0      136479.0      257169 Loss 152895.0     257241 Loss 139816.0      257288 Loss 202768.0      257614 Loss 309901.0     Loss 324417.0     Loss 191948.0           Loss 396916.0      258414 Loss 141584.0     258493 Loss 121843.0     Loss 185302.0      258626 Loss 146527.0      Loss 287409.0          258948 Loss 393459.0      259093 Loss 94125.7      Loss 319092.0      259216 Loss 126765.0      Loss 219920.0      259382 Loss 130196.0      259744 Loss 118189.0         259983 Loss 214554.0      308168.0      260152 Loss 208960.0      260338 Loss 138063.0      260412 Loss 417634.0      260526 Loss 41291.1          261150 Loss 94377.2      261241 Loss 268405.0      219882.0      261376 Loss 198679.0      261464 Loss 399748.0      261612 Loss 340309.0          261901 Loss 224081.0      261950 Loss 137948.0           205957.0     262466 Loss 102134.0     262528 Loss 313158.0     262867 Loss 187307.0      297063.0      263231 Loss 268169.0      Loss 414688.0      263363 Loss 221930.0      263624 Loss 179838.0      245131.0     263793 Loss 215823.0      153626.0      263963 Loss 125738.0      264033 Loss 280658.0      264116 Loss 39649.3     264233 Loss 302758.0      264394 Loss 298696.0          264549 Loss 327601.0     264686 Loss 355020.0      264757 Loss 281530.0      264860 Loss 265740.0      264954 Loss 108442.0      208442.0     Loss 226304.0     Loss 233609.0      265344 Loss 141273.0      257829.0      265454 Loss 113088.0      Loss 146685.0      265601 Loss 287335.0     265666 Loss 141233.0      265717 Loss 125042.0           265976 Loss 155412.0      266235 Loss 152014.0               266544 Loss 241461.0      266635 Loss 281196.0      266730 Loss 191706.0      64963.6         122427.0               267493 Loss 319110.0     267847 Loss 292349.0      267953 Loss 214971.0      268029 Loss 164668.0     200516.0      268196 Loss 142982.0      268283 Loss 242915.0      268415 Loss 201059.0      268644 Loss 363168.0     268701 Loss 245017.0      268770 Loss 410554.0      269077 Loss 217974.0          269333 Loss 216078.0      269894 Loss 296721.0      Loss 226647.0     270258 Loss 166623.0      Loss 316146.0      270508 Loss 177792.0      270666 Loss 128075.0      270724 Loss 208807.0          270870 Loss 347475.0      271094 Loss 142995.0      271150 Loss 147474.0      Loss 183825.0      271327 Loss 210248.0      271379 Loss 209311.0         268836.0     226758.0      271891 Loss 327217.0     272089 Loss 159917.0      237512.0      272379 Loss 248758.0      150476.0      272703 Loss 275675.0     240423.0      206916.0     273158 Loss 122552.0          273496 Loss 270903.0      Loss 96223.4     Loss 476253.0     Loss 152930.0      273918 Loss 154595.0      274066 Loss 172794.0      274217 Loss 225310.0      274281 Loss 176299.0               274772 Loss 151762.0          224662.0      275562 Loss 146943.0      Loss 186654.0               275926 Loss 185896.0      276375 Loss 283747.0      276577 Loss 234760.0      276728 Loss 148051.0     276827 Loss 181672.0          Loss 60122.2      277434 Loss 218119.0          154222.0           277944 Loss 133917.0      278104 Loss 197706.0      278111 Loss 322347.0      278139 Loss 264385.0      164076.0      278291 Loss 174860.0      204820.0      278504 Loss 68437.0      278587 Loss 244479.0      Loss 98172.9      279107 Loss 175962.0      Loss 207894.0      Loss 227655.0      Loss 204852.0      279671 Loss 106575.0      280015 Loss 290190.0     \r"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #plt.ion()\n",
    "    #plt.figure()\n",
    "    #plt.show()\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        x,y = samples_generator(np_add, (num_samples, num_features) , (0, 100))\n",
    "        _current_state = np.zeros((batch_size, state_size))\n",
    "\n",
    "        #print(\"\\r New data, epoch\", epoch_idx)\n",
    "\n",
    "        #for batch_idx in range(num_batches):\n",
    "            #start_idx = batch_idx * truncated_backprop_length\n",
    "            #end_idx = start_idx + truncated_backprop_length\n",
    "\n",
    "        #batchX = x[:,start_idx:end_idx]\n",
    "        #batchY = y[:,start_idx:end_idx]\n",
    "\n",
    "        _total_loss, _train_step, _current_state, _outputs = sess.run(\n",
    "                [total_loss, train_step, current_state, outputs],\n",
    "                feed_dict={\n",
    "                    init_state:_current_state,\n",
    "                    batchX_placeholder:x,\n",
    "                    batchY_placeholder:y\n",
    "                })\n",
    "        loss_list.append(_total_loss)\n",
    "            \n",
    "        print(\"Epoch\",epoch_idx, \"Loss\", _total_loss , \"    \", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
